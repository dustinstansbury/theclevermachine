<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="stylesheet" href="/theclevermachine/assets/main.css"">
  <link rel=" icon" type="image/png" href="icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>p-Hacking 101: N Chasing | The Clever Machine</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="p-Hacking 101: N Chasing" />
<meta name="author" content="Dustin Stansbury" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="”(N) Chasing,” or adding new observations to an already-analyzed experiment can increase your experiment’s false positive rate. As an experimenter or analyst, you may have heard of the dangers of (N) chasing, but may not have an intuition as to why or how it increases Type I Error. In this post we’ll demonstrate (N) chasing using some simulations, and show that, under certain settings, adding just a single data point to your experiment can dramatically increase false positives." />
<meta property="og:description" content="”(N) Chasing,” or adding new observations to an already-analyzed experiment can increase your experiment’s false positive rate. As an experimenter or analyst, you may have heard of the dangers of (N) chasing, but may not have an intuition as to why or how it increases Type I Error. In this post we’ll demonstrate (N) chasing using some simulations, and show that, under certain settings, adding just a single data point to your experiment can dramatically increase false positives." />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/p-hacking-n-chasing" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/p-hacking-n-chasing" />
<meta property="og:site_name" content="The Clever Machine" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-04T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="p-Hacking 101: N Chasing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Dustin Stansbury"},"dateModified":"2020-10-04T00:00:00-07:00","datePublished":"2020-10-04T00:00:00-07:00","description":"”(N) Chasing,” or adding new observations to an already-analyzed experiment can increase your experiment’s false positive rate. As an experimenter or analyst, you may have heard of the dangers of (N) chasing, but may not have an intuition as to why or how it increases Type I Error. In this post we’ll demonstrate (N) chasing using some simulations, and show that, under certain settings, adding just a single data point to your experiment can dramatically increase false positives.","headline":"p-Hacking 101: N Chasing","mainEntityOfPage":{"@type":"WebPage","@id":"https://dustinstansbury.github.io/theclevermachine/p-hacking-n-chasing"},"url":"https://dustinstansbury.github.io/theclevermachine/p-hacking-n-chasing"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a><a class="page-link" href="/theclevermachine/topics/">Topics</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">p-Hacking 101: N Chasing</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-10-04T00:00:00-07:00" itemprop="datePublished"><i class="fa-solid fa-calendar"></i> Oct 4, 2020<br>
        <i class="fa-solid fa-scissors"></i> Dec 3, 2023</time>
      <br><span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card"
          itemprop="name"> <i class="fa-solid fa-pencil"> </i> Dustin Stansbury
        </span>
      </span><br>
      <i class="fa-solid fa-tags"></i><span itemprop="tags">
        
        
        
        <a href="/theclevermachine/topics/statistics.html">statistics</a>
        
        
        <a href="/theclevermachine/topics/hypothesis-testing.html">hypothesis-testing</a>
        
        
        <a href="/theclevermachine/topics/ab-testing.html">ab-testing</a>
        
        
        <a href="/theclevermachine/topics/false-positive.html">false-positive</a>
        
        
        <a href="/theclevermachine/topics/type-I-error.html">type-I-error</a>
        
        
        <a href="/theclevermachine/topics/p-hacking.html">p-hacking</a>
        
        
        

      </span></p>

  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>”\(N\) Chasing,” or adding new observations to an already-analyzed experiment can increase your experiment’s false positive rate. As an experimenter or analyst, you may have heard of the dangers of \(N\) chasing, but may not have an intuition as to why or how it increases Type I Error. In this post we’ll demonstrate \(N\) chasing using some simulations, and show that, under certain settings, adding just a single data point to your experiment can dramatically increase false positives.</p>

<h1 id="well-behaved-statistical-tests">Well-behaved Statistical Tests</h1>

<p>A well-behaved statistical test should provide uniformly-distributed p-values. This is because the test shouldn’t favor any one portion of the hypothesis space over the others. This is demonstrated in <strong><em>Figure 1</em></strong>, which plots the distribution of <em>p</em>-values that result from running two-sample <em>t</em>-tests on 10,000 simulated datasets (\(N=10\)) having no difference between the two samples being tested, i.e. the Null Hypothesis \(H_0=\text{True}\).</p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="600" src="assets/images/p-hacking-n-chasing/well-behaved-statistical-test.png" />
    </div>
</center>

<p><strong><em>Figure 1, p-values from a well-behaved statistical test.</em></strong> <em>p-values should be uniformly distributed; here we choose twenty equally-sized bins, corresponding with \(\alpha=0.05\). Even when there is no effect, i.e. \(H_0=\text{True}\), 5% of trials will indicate a “significant” effect by chance (red). Additionally, 5% of trials will be “So close” to showing significance (blue). N chasing is often performed on these “So close” trials by collecting additional data points.</em></p>

<details>
  <summary><b>Figure 1</b> Python Code</summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">abra.vis</span> <span class="kn">import</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">COLORS</span>  <span class="c1"># requires abracadabra
</span><span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Rectangle</span>

<span class="c1"># Simulate multiple experimental datasets where H_0=True
# run t-tests, then collect the resulting p-values
</span><span class="n">ALPHA</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">n_obs_per_trial</span><span class="p">,</span> <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10000</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">null</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">()</span>
<span class="n">datasets</span> <span class="o">=</span> <span class="n">null</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_obs_per_trial</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span>
<span class="n">pvals</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">pvalue</span>

<span class="k">def</span> <span class="nf">pval_rate_histogram</span><span class="p">(</span><span class="n">pvals</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""Util for plotting the number of p-values that occur within buckets
    of size `resolution`.
    """</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">factor</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pvals</span><span class="p">))</span>
    <span class="n">cnts</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">pvals</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">factor</span><span class="o">*</span><span class="n">cnts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>

<span class="c1"># Plot distribution of non-hacked p-values
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">cnt</span><span class="p">,</span> <span class="n">bin_left</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">pval_rate_histogram</span><span class="p">(</span><span class="n">pvals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">light_gray</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'p-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="mi">07</span><span class="p">])</span>

<span class="c1"># Highlight the trials bucket associated with false positives as
# well as those trials that are "Soo close" to being "significant"
</span>
<span class="c1">## False positives trials
</span><span class="n">expected_type_I</span> <span class="o">=</span> <span class="n">patches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">expected_type_I</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">COLORS</span><span class="p">.</span><span class="n">red</span><span class="p">)</span>
<span class="n">expected_type_I_rate</span> <span class="o">=</span> <span class="n">cnt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">100.</span>
<span class="n">expected_type_I</span><span class="p">.</span><span class="n">set_label</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">expected_type_I_rate</span><span class="p">)</span><span class="si">}</span><span class="s">% of Trials are False Positives"</span><span class="p">)</span>

<span class="c1">## So close to being "significant" trials
</span><span class="n">near_type_I</span> <span class="o">=</span> <span class="n">patches</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">near_type_I</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">COLORS</span><span class="p">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">near_type_I</span><span class="p">.</span><span class="n">set_label</span><span class="p">(</span><span class="s">"'Soo close!' Trials"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">dark_red</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Expected Type I Error Rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'p-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Proportion of Trials'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"p-values from a well-behaved statistical test are uniform"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div>  </div>
</details>

<hr />
<p><br /></p>

<p>Because the <em>p</em>-vlaues are uniformly distributed, if you histogram the <em>p</em>-values into 20 equally-sized bins, you would expect each bin to be associated with roughly 5% of trials. Consequently, we would expect a default false positive rate \(\alpha\) of 0.05. It turns out this resolution of <em>p</em>-value breakdown that is a pretty common scientific standard and is one of the reasons everyone uses an \(\alpha=0.05\) in hypothesis tests.</p>

<h1 id="n-chasing"><em>N</em> Chasing</h1>

<p><strong><em>Figure 1</em></strong> also highlights in blue the trials where the <em>p</em>-values are “So close” to exhibiting a significant effect, having magnitudes just above the \(\alpha=0.05\) cutoff.</p>

<p>If you were an experimenter, who is incentivised to find novel, positive effects in your experiment–even though there isn’t one, as is the case here, but you don’t know that–you might be tempted to just extend your experiment <em>juuuust a liiiiittle</em> longer to see if the <em>p</em>-values for those “So close” trials decrease enough to reach statistical significance.</p>

<p>At first glance, adding new samples in this way seems totally reasonable. How can adding more data be bad; if the effect is there, then we should be able see it better by simply “squashing down the noise” with more samples, right? <strong>This is <em>N</em> chasing, a common form of <em>p</em>-hacking, don’t do it!</strong></p>

<p><br /></p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="600" src="assets/images/p-hacking-n-chasing/p-hacking-via-n-chasing.png" />
    </div>
</center>

<p><strong><em>Figure 2, p-Hacking via N Chasing.</em></strong> <em>To simulate N Chasing, we take the “So close” (blue) trials in Figure 1 and add to each trial a single, random data point drawn from \(H_0\) (\(N_{hacked}=11\)). The resulting distribution of p-values from running two-sample t-tests on the hacked datasets is shown. The distribution is no longer uniform–the sign of a ill-behaved statistical test. Additionally, the Type I error rate is around 25% (red bar), where we would expect false positives in around 5% of trials (dark red line).</em></p>

<details>
  <summary><b>Figure 2</b> Python Code</summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Now hack the "So close" trials by adding samples to the H_0 dataset
## Identify the so-close trials and p-values
</span><span class="n">hack_index_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">pvals</span> <span class="o">&gt;=</span> <span class="mf">0.05</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pvals</span> <span class="o">&lt;</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hacked_datasets</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">hack_index_mask</span><span class="p">]</span>
<span class="n">n_hacked_trials</span> <span class="o">=</span> <span class="n">hacked_datasets</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1">## Add samples and re-run tests, collecting new p-values
</span><span class="n">n_additional_samples</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">hacked_datasets</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">hacked_datasets</span><span class="p">,</span> <span class="n">null</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_additional_samples</span><span class="p">,</span> <span class="n">n_hacked_trials</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">hacked_pvals</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">hacked_datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hacked_datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">pvalue</span>

<span class="c1"># Display resulting hacked p-values distribution
</span><span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">hacked_cnt</span><span class="p">,</span> <span class="n">hacked_bin_left</span><span class="p">,</span> <span class="n">hacked_patches</span> <span class="o">=</span> <span class="n">pval_rate_histogram</span><span class="p">(</span><span class="n">hacked_pvals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">inflated_type_I</span> <span class="o">=</span> <span class="n">hacked_patches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">inflated_type_I</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">COLORS</span><span class="p">.</span><span class="n">red</span><span class="p">)</span>
<span class="n">inflated_type_I_rate</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">hacked_cnt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">inflated_type_I</span><span class="p">.</span><span class="n">set_label</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">inflated_type_I_rate</span><span class="p">)</span><span class="si">}</span><span class="s">% of Trials are False Positives"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">ALPHA</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">dark_red</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Expected Type I Error Rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'p-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Proportion of Trials'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"p-values of 'Soo close!' trials after adding </span><span class="si">{</span><span class="n">n_additional_samples</span><span class="si">}</span><span class="s"> additional datapoint(s)"</span><span class="p">)</span>
</code></pre></div>  </div>
</details>

<hr />
<p><br /></p>

<p>To demonstrate how hacking <em>p</em>-values via <em>N</em> chasing inflates false positive rates, we take the “So close” (blue) trials from the simulation in <strong><em>Figure 1</em></strong>, and add to each trial a random data point drawn from the \(H_0\) distribution. We then re-run our two-sample <em>t</em>-tests and histogram the resulting <em>p</em>-values.</p>

<p><strong><em>Figure 2</em></strong> shows the resulting distribution of hacked <em>p</em>-values. These trials originally exhibited a False Positive Rate of 0% (i.e. they did not fall into the \(p \le \alpha = 0.05\) bin). However, these trials now exhibit a Type I error rate over 25% (red), nearly 5 times the expected false positive rate 5% (dark red line)! Just from adding <strong>a single data point</strong> to those trials!</p>

<p>Another piece of evidence suggesting that something has gone awry is that the distribution of <em>p</em>-values on these augmented trials is no longer uniform, but right-skewed. Thus the statistical test on these data is no longer unbiased, instead favoring lower <em>p</em>-values.</p>

<p>The problem here is that we’re adding information into the system by first calculating test statistics/<em>p</em>-values, interpreting the results, then deciding to add more data and testing again. It turns out that this is a flavor of statistical error known as the <em>Multiple Comparisons Problem.</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>It’s worth noting that the simulation presented here is based on a pretty small sample size of \(N=10\). Thus, adding a single data point has a much larger effect on Type I error rate than it might for larger sample sizes. However, the effect is consistent on larger \(N\) as well if one is adding new samples to the experiment that are in proportion to \(N\).</p>

<h1 id="wrapping-up">Wrapping Up</h1>

<p><em>N</em> chasing is just one of many spooky gotchas that come along with using Null hypothesis-based statistical tests (NHST). This particular <em>p</em>-hacking effect comes up when you know that you’ve run the experiment, did not reach significance, then decide to keep running the experiment after looking at the results. If you’ve ever said something like “oh, let’s just run it a little longer,” then you’re probably p-hacking.</p>

<p>The negative affects of <em>N</em> chasing can be minimized by sticking to standardized protocols for running experiments that use NHSTs: running an initial <a href="https://en.wikipedia.org/wiki/Sample_size_determination">power analysis</a> to calculate the required sample size for a desired <a href="https://en.wikipedia.org/wiki/Effect_size">effect size</a> and <a href="https://en.wikipedia.org/wiki/Power_of_a_test">statistical power</a>, then stopping your data collection once you’ve reached the requirements prescribed by the power analysis. Continuing to collect data beyond what is prescribed will inflate your Type I error rate, and likely provide misleading results for your experiment.</p>

<hr />
<hr />
<h1 id="references">References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01444/full">Szucs, D., A Tutorial on Hunting Statistical Significance by Chasing N (2016)</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'the-clever-machine'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/theclevermachine/p-hacking-n-chasing" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
