<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>The Clever Machine - neural-networks</title>
    <link href="/theclevermachine" rel="self"/>
    <link href="tagsneural-networks.html"/>
    <updated>2020-07-02T22:44:29-07:00</updated>
    <id>/theclevermachineneural-networks.html</id>

    <author>
        <name>Dustin Stansbury</name>
    </author>

    
        <entry>
            <title>Derivation: Derivatives for Common Neural Network Activation Functions</title>
            <link href="/theclevermachine/derivation-common-neural-network-activation-functions"/>
            <updated>2020-06-29T00:00:00-07:00</updated>
            <id>/theclevermachine/derivation-common-neural-network-activation-functions</id>
            <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;When constructing Artificial Neural Network (ANN) models, one of the primary considerations is choosing activation functions for hidden and output layers that are differentiable. This is because calculating the backpropagated error signal that is used to determine ANN parameter updates requires the gradient of the activation function gradient . Three of the most commonly-used activation functions used in ANNs are the identity function, the logistic sigmoid function, and the hyperbolic tangent function. Examples of these functions and their associated gradients (derivatives in 1D) are plotted in Figure 1.&lt;/p&gt;

&lt;hr /&gt;
&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;800&quot; src=&quot;assets/images/a-gentle-introduction-to-neural-networks/common_activation_functions.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1:&lt;/em&gt;&lt;/strong&gt; Common activation functions functions used in artificial neural, along with their derivatives&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In the remainder of this post, we derive the derivatives/gradients for each of these common activation functions.&lt;/p&gt;

&lt;h1 id=&quot;the-identity-activation-function&quot;&gt;The Identity Activation Function&lt;/h1&gt;

&lt;p&gt;The simplest activation function, one that is commonly used for the output layer activation function in regression problems,  is the identity/linear activation function (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, red curves):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_{linear}(z) = z&lt;/script&gt;

&lt;p&gt;This activation function simply maps the pre-activation to itself and can output values that range &lt;script type=&quot;math/tex&quot;&gt;(-\infty, \infty)&lt;/script&gt;. Why would one want to do use an identity activation function? After all, a multi-layered network with linear activations at each layer can be equally-formulated as a single-layered linear network. It turns out that the identity activation function is surprisingly useful. For example, a multi-layer network that has nonlinear activation functions amongst the hidden units and an output layer that uses the identity activation function implements a powerful form of nonlinear regression. Specifically, the network can predict continuous target values using a linear combination of signals that arise from one or more layers of nonlinear transformations of the input.&lt;/p&gt;

&lt;p&gt;The derivative of &lt;script type=&quot;math/tex&quot;&gt;g_{\text{linear}}&lt;/script&gt; ,  &lt;script type=&quot;math/tex&quot;&gt;g'_{\text{linear}}&lt;/script&gt;,  is simply 1, in the case of 1D inputs. For vector inputs of length D the gradient is &lt;script type=&quot;math/tex&quot;&gt;\vec{1}^{1 \times D}&lt;/script&gt;, a vector of ones of length D.&lt;/p&gt;

&lt;h1 id=&quot;the-logistic-sigmoid-activation-function&quot;&gt;The Logistic Sigmoid Activation Function&lt;/h1&gt;

&lt;p&gt;Another function that is often used as the output activation function for binary classification problems (i.e. outputs values that range (0, 1)), is the logistic sigmoid (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, blue curves). The logistic sigmoid has the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{rcl} g_{\text{logistic}}(z) = \frac{1}{1 + e^{-z}}\end{array}&lt;/script&gt;

&lt;p&gt;and outputs values that range (0, 1). The logistic sigmoid is motivated somewhat by biological neurons and can be interpreted as the probability of an artificial neuron “firing” given its inputs. (It turns out that the logistic sigmoid can also be derived as the maximum likelihood solution to for logistic regression in statistics). Calculating the derivative of the logistic sigmoid function makes use of the quotient rule and a clever trick that both adds and subtracts a one from the numerator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g'_{\text{logistic}}(z) &amp;=&amp; \frac{\partial}{\partial z} \left ( \frac{1}{1 + e^{-z}}\right ) \\  &amp;=&amp; \frac{e^{-z}}{(1 + e^{-z})^2} \text{(by chain rule)} \\  &amp;=&amp; \frac{1 + e^{-z} - 1}{(1 + e^{-z})^2} \\  &amp;=&amp; \frac{1 + e^{-z}}{(1 + e^{-z})^2} - \left ( \frac{1}{1+e^{-z}} \right )^2 \\  &amp;=&amp; \frac{1}{(1 + e^{-z})} - \left ( \frac{1}{1+e^{-z}} \right )^2 \\  &amp;=&amp; g_{\text{logistic}}(z)- g_{\text{logistic}}(z)^2 \\  &amp;=&amp; g_{\text{logistic}}(z)(1 - g_{\text{logistic}}(z)) \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here we see that &lt;script type=&quot;math/tex&quot;&gt;g'_{logistic}(z)&lt;/script&gt; evaluated at &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is simply &lt;script type=&quot;math/tex&quot;&gt;g_{logistic}(z)&lt;/script&gt; weighted by &lt;script type=&quot;math/tex&quot;&gt;(1-g_{logistic}(z))&lt;/script&gt;. This turns out to be a convenient form for efficiently calculating gradients used in neural networks: if one keeps in memory the feed-forward activations of the logistic function for a given layer, the gradients for that layer can be evaluated using simple multiplication and subtraction rather than performing any re-evaluating the sigmoid function, which requires extra exponentiation.&lt;/p&gt;

&lt;h1 id=&quot;the-hyperbolic-tangent-activation-function&quot;&gt;The Hyperbolic Tangent Activation Function&lt;/h1&gt;

&lt;p&gt;Though the logistic sigmoid has a nice biological interpretation, it turns out that the logistic sigmoid can cause a neural network to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since neural networks use the feed-forward activations to calculate parameter gradients (again, see this &lt;a href=&quot;https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/&quot;&gt;this post&lt;/a&gt; for details), this can result in model parameters that are updated less regularly than we would like, and are thus “stuck” in their current state.&lt;/p&gt;

&lt;p&gt;An alternative to the logistic sigmoid is the hyperbolic tangent, or &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}&lt;/script&gt; function (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, green curves):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g_{\text{tanh}}(z) &amp;=&amp; \frac{\text{sinh}(z)}{\text{cosh}(z)} \\  &amp;=&amp; \frac{\mathrm{e}^z - \mathrm{e}^{-z}}{\mathrm{e}^z + \mathrm{e}^{-z}}\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Like the logistic sigmoid, the tanh function is also sigmoidal (“s”-shaped), but instead outputs values that range &lt;script type=&quot;math/tex&quot;&gt;(-1, 1)&lt;/script&gt;. Thus strongly negative inputs to the tanh will map to negative outputs. Additionally, only zero-valued inputs are mapped to near-zero outputs. These properties make the network less likely to get “stuck” during training. Calculating the gradient for the tanh function also uses the quotient rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g'_{\text{tanh}}(z) &amp;=&amp; \frac{\partial}{\partial z} \frac{\text{sinh}(z)}{\text{cosh}(z)} \\  &amp;=&amp; \frac{\frac{\partial}{\partial z} \text{sinh}(z) \times \text{cosh}(z) - \frac{\partial}{\partial z} \text{cosh}(z) \times \text{sinh}(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; \frac{\text{cosh}^2(z) - \text{sinh}^2(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; 1 - \frac{\text{sinh}^2(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; 1 - \text{tanh}^2(z)\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similar to the derivative for the logistic sigmoid, the derivative of &lt;script type=&quot;math/tex&quot;&gt;g_{\text{tanh}}(z)&lt;/script&gt; is a function of feed-forward activation evaluated at z, namely &lt;script type=&quot;math/tex&quot;&gt;(1-g_{\text{tanh}}(z)^2)&lt;/script&gt;. Thus the same caching trick can be used for layers that implement &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}&lt;/script&gt; activation functions.&lt;/p&gt;

&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;In this post we reviewed a few commonly-used activation functions in neural network literature and their derivative calculations. These activation functions are motivated by biology and/or provide some handy implementation tricks like calculating derivatives using cached feed-forward activation values. Note that there are also many other options for activation functions not covered here: e.g. rectification, soft rectification, polynomial kernels, etc. Indeed, finding and evaluating novel activation functions is an active subfield of machine learning research. However, the three basic activations covered here can be used to solve a majority of the machine learning problems one will likely face.&lt;/p&gt;
</content>
        </entry>
    
</feed>
