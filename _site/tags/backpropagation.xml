<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>The Clever Machine - backpropagation</title>
    <link href="/theclevermachine" rel="self"/>
    <link href="tagsbackpropagation.html"/>
    <updated>2020-07-02T00:56:59-07:00</updated>
    <id>/theclevermachinebackpropagation.html</id>

    <author>
        <name>Dustin Stansbury</name>
    </author>

    
        <entry>
            <title>A Gentle Introduction to Artificial Neural Networks</title>
            <link href="/theclevermachine/a-gentle-introduction-to-neural-networks"/>
            <updated>2020-06-30T00:00:00-07:00</updated>
            <id>/theclevermachine/a-gentle-introduction-to-neural-networks</id>
            <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Though many phenomena in the world can be well-modeled using basic linear regression or classification, there are also many interesting phenomena that are nonlinear in nature. In order to deal with nonlinear phenomena, there have been a diversity of nonlinear models developed.&lt;/p&gt;

&lt;p&gt;For example parametric models assume that data follow some parameteric class of nonlinear function (e.g. polynomial, power, or exponential), then fine-tune the shape of the parametric function to fit observed data. However this approach is only helpful if data are fit nicely by the available catalog of parametric functions.&lt;/p&gt;

&lt;p&gt;Another approach, kernel-based methods, transforms data non-linearly into an abstract space that measures distances between observations, then predicts new values or classes based on these distances. However, kernel methods generally involve constructing a kernel matrix that depends on the number of training observations and can thus be prohibitive for large data sets.&lt;/p&gt;

&lt;p&gt;Another class of models, the ones that are the focus of this post, are artificial neural networks (ANNs). ANNs are nonlinear models motivated by the physiological architecture of the nervous system. They involve a cascade of simple nonlinear computations that, when aggregated, can implement robust and complex nonlinear functions. In fact, depending on how they are constructed, ANNs can approximate any nonlinear function, making them a quite powerful class of models&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In recent years ANNs that use multiple stages of nonlinear computation (aka “deep learning”)  have been able obtain outstanding performance on an array of complex tasks ranging from visual object recognition to natural language processing. I find ANNs super interesting due to their computational power and their intersection with computational neuroscience.  However, I’ve found that most of the available tutorials on ANNs are either dense with formal details and contain little information about implementation or any examples, while others skip a lot of the mathematical detail and provide implementations that seem to come from thin air.  This post aims at giving a more complete overview of ANNs, including (varying degrees of) the math behind ANNs, how ANNs are implemented in code, and finally some toy examples that point out the strengths and weaknesses of ANNs.&lt;/p&gt;

&lt;h1 id=&quot;single-layer-neural-networks&quot;&gt;Single-layer Neural Networks&lt;/h1&gt;

&lt;p&gt;The simplest ANN (Figure 1) takes a set of observed inputs &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}=(a_1, a_2, ..., a_N)&lt;/script&gt;, multiplies each of them by their own associated weight &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w} = (w_1, w_2, ...w_N)&lt;/script&gt; , and sums the weighted values to form a pre-activation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.  Oftentimes there is also a bias &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; that is tied to an input that is always +1 included in the preactivation calculation. The network then transforms the pre-activation using a nonlinear activation function &lt;script type=&quot;math/tex&quot;&gt;g(z)&lt;/script&gt; to output a final activation &lt;script type=&quot;math/tex&quot;&gt;a_{\text{out}}&lt;/script&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;320&quot; src=&quot;assets/images/a-gentle-introduction-to-neural-networks/perceptron2.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;: Diagram of a single-layered artificial neural network.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;There are many options available for the form of the activation function g(z), and the choice generally depends on the task we would like the network to perform. For instance, if the activation function is the identity function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{rcl}g_{\text{linear}}(z) = z\end{array},&lt;/script&gt;

&lt;p&gt;which outputs continuous values &lt;script type=&quot;math/tex&quot;&gt;a_{linear}\in (-\infty, \infty)&lt;/script&gt;, then the network implements a linear model akin to used in standard linear regression. Another choice for the activation function is the logistic sigmoid:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{rcl}g_{\text{logistic}}(z) = \frac{1}{1+e^{-z}}\end{array},&lt;/script&gt;

&lt;p&gt;which outputs values &lt;script type=&quot;math/tex&quot;&gt;a_{logistic} \in (0,1)&lt;/script&gt;. When the network outputs use the logistic sigmoid activation function, the network implements linear binary classification. Binary classification can also be implemented using the hyperbolic tangent function, &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}(z)&lt;/script&gt;, which outputs values &lt;script type=&quot;math/tex&quot;&gt;a_{\text{tanh}}\in (-1, 1)&lt;/script&gt; (note that the classes must also be coded as either -1 or 1 when using &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}&lt;/script&gt;. Single-layered neural networks used for classification are often referred to as “perceptrons,” a name given to them when they were first developed in the late 1950s.&lt;/p&gt;

&lt;hr /&gt;
&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;680&quot; src=&quot;assets/images/a-gentle-introduction-to-neural-networks/activation_functions.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 2:&lt;/em&gt;&lt;/strong&gt; Common activation functions functions used in artificial neural, along with their derivatives&lt;/p&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# % DEFINE A FEW COMMON ACTIVATION FUNCTIONS
# gLinear = inline('z','z');
# gSigmoid = inline('1./(1+exp(-z))','z');
# gTanh = inline('tanh(z)','z');
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

 
&lt;span class=&quot;c1&quot;&gt;# % ...DEFINE THEIR DERIVATIVES
# gPrimeLinear = inline('ones(size(z))','z');
# gPrimeSigmoid = inline('1./(1+exp(-z)).*(1-1./(1+exp(-z)))','z');
# gPrimeTanh = inline('1-tanh(z).^2','z');
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_prime_linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_prime_sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_prime_tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# % VISUALIZE EACH g(z)
# z = linspace(-4,4,100);
# figure
# set(gcf,'Position',[100,100,960,420])
# subplot(121);  hold on;
# h(1) = plot(z,gLinear(z),'r','Linewidth',2);
# h(2) = plot(z,gSigmoid(z),'b','Linewidth',2);
# h(3) = plot(z,gTanh(z),'g','Linewidth',2);
# set(gca,'fontsize',16)
# xlabel('z')
# legend(h,{'g_{linear}(z)','g_{logistic}(z)','g_{tanh}(z)'},'Location','Southeast')
# title('Some Common Activation Functions')
# hold off, axis square, grid
# ylim([-1.1 1.1])
&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# % VISUALIZE EACH g'(z)
# subplot(122); hold on
# h(1) = plot(z,gPrimeLinear(z),'r','Linewidth',2);
# h(2) = plot(z,gPrimeSigmoid(z),'b','Linewidth',2);
# h(3) = plot(z,gPrimeTanh(z),'g','Linewidth',2);
# set(gca,'fontsize',16)
# xlabel('z')
# legend(h,{'g''_{linear}(z)','g''_{logistic}(z)','g''_{tanh}(z)'},'Location','South')
# title('Activation Function Derivatives')
# hold off, axis square, grid
# ylim([-.5 1.1])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;NOTE This post is a refactor of content with the same title originally posted on &lt;a href=&quot;https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/&quot;&gt;The Clever Machine&lt;/a&gt; wordpress blog.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;This property is not reserved for ANNs; kernel methods are also considered “universal approximators”; however, it turns out that neural networks with multiple layers are more efficient at approximating arbitrary functions than other methods. I refer the interested reader to &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf&quot;&gt;an in-depth discussion&lt;/a&gt; on the topic.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
        </entry>
    
</feed>
