<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="stylesheet" href="/theclevermachine/assets/main.css"">
  <link rel=" icon" type="image/png" href="icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>p-Hacking 101: Data Peeking | The Clever Machine</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="p-Hacking 101: Data Peeking" />
<meta name="author" content="Dustin Stansbury" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="“Data peeking” is the process of prematurely running statistical tests on your AB experiment data before data collection has reached the required sample size prescribed by power analysis. You may have heard of the dangers of data peeking, but may not have an intuition as to how dramatically it can inflate your False Positive rate, and thus mislead statistical inferences. In this post we’ll use simulations to demonstrate just how much data peeking can inflate false positives." />
<meta property="og:description" content="“Data peeking” is the process of prematurely running statistical tests on your AB experiment data before data collection has reached the required sample size prescribed by power analysis. You may have heard of the dangers of data peeking, but may not have an intuition as to how dramatically it can inflate your False Positive rate, and thus mislead statistical inferences. In this post we’ll use simulations to demonstrate just how much data peeking can inflate false positives." />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/p-hacking-data-peeking" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/p-hacking-data-peeking" />
<meta property="og:site_name" content="The Clever Machine" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-13T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="p-Hacking 101: Data Peeking" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Dustin Stansbury"},"dateModified":"2023-12-13T00:00:00-08:00","datePublished":"2023-12-13T00:00:00-08:00","description":"“Data peeking” is the process of prematurely running statistical tests on your AB experiment data before data collection has reached the required sample size prescribed by power analysis. You may have heard of the dangers of data peeking, but may not have an intuition as to how dramatically it can inflate your False Positive rate, and thus mislead statistical inferences. In this post we’ll use simulations to demonstrate just how much data peeking can inflate false positives.","headline":"p-Hacking 101: Data Peeking","mainEntityOfPage":{"@type":"WebPage","@id":"https://dustinstansbury.github.io/theclevermachine/p-hacking-data-peeking"},"url":"https://dustinstansbury.github.io/theclevermachine/p-hacking-data-peeking"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a><a class="page-link" href="/theclevermachine/topics/">Topics</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">p-Hacking 101: Data Peeking</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-12-13T00:00:00-08:00" itemprop="datePublished"><i class="fa-solid fa-calendar"></i> Dec 13, 2023</time>
      <br><span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card"
          itemprop="name"> <i class="fa-solid fa-pencil"> </i> Dustin Stansbury
        </span>
      </span><br>
      <i class="fa-solid fa-tags"></i><span itemprop="tags">
        
        
        
        <a href="/theclevermachine/topics/statistics.html">statistics</a>
        
        
        <a href="/theclevermachine/topics/hypothesis-testing.html">hypothesis-testing</a>
        
        
        <a href="/theclevermachine/topics/ab-testing.html">ab-testing</a>
        
        
        <a href="/theclevermachine/topics/false-positive.html">false-positive</a>
        
        
        <a href="/theclevermachine/topics/type-I-error.html">type-I-error</a>
        
        
        <a href="/theclevermachine/topics/p-hacking.html">p-hacking</a>
        
        
        

      </span></p>

  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em>“Data peeking”</em> is the process of prematurely running statistical tests on your AB experiment data before data collection has reached the required sample size prescribed by <a href="https://en.wikipedia.org/wiki/Sample_size_determination">power analysis</a>. You may have heard of the dangers of data peeking, but may not have an intuition as to how dramatically it can inflate your False Positive rate, and thus mislead statistical inferences. In this post we’ll use simulations to demonstrate just how much data peeking can inflate false positives.</p>

<h1 id="background-null-hypothesis-statistical-tests-and-type-i-error-rates">Background: Null-hypothesis Statistical Tests and Type I Error Rates</h1>

<p>A common experimental method used for AB testing is Null hypothesis-based statistical testing (NHST). In the NHST approach, the scientist collects data from some process or group of interest, then performs a statistical test to evaluate whether the distribution of collected samples is statistically different from the distribution generated by some baseline–aka “Null”–process or control group.</p>

<p>Due to inherent randomness in the world and the fact that sampling procedures will always be imperfect, it is possible that the researcher detects a statistically significant difference when, in fact, there isn’t one. This scenario is called a <strong>False Positive</strong> or <strong>Type I Error</strong>, and each NHST has its own acceptable Type I Error rate. The acceptable False Positive rate is generally referred to as \(\alpha\), and tends to be set in the range of \(1 \%-5 \%\).</p>

<p>A key step when using NHST is to establish a minimum number of samples \(N\) to collect in order to provide statistical guarantees on the expected False Positive rate. This procedure is know as <a href="https://en.wikipedia.org/wiki/Power_of_a_test">power analysis</a>.</p>

<h1 id="data-peeking-and-inflating-false-positive-rate">Data-peeking and inflating False Positive rate</h1>

<p>Sometimes, during the data collection process, the scientist will “peek” at the data prematurely, before data collection has reached the \(N\) samples prescribed by power analysis. The scientist will then perform preliminary analyses and statistical tests on this incomplete dataset. <strong>This is where the p-hacking begins!</strong></p>

<p>Say the scientist finds no statistical difference from the Null hypothesis. In this case, it highly likely that the scientist will collect more data in order to try to further establish whetehr there is a real difference. However, if a difference <em>is</em> detected, data collection is likely to be stopped–a form of confirmation bias, if you will. <strong>This strategy of peeking and conditionally continuing data collection based on test results obtained from the partial dataset invalidates the assumptions of the statistical test and inflates the likelihood of observing a false positive.</strong></p>

<h1 id="simulating-the-effects-of-data-peeking-on-false-positive-rate">Simulating the effects of Data Peeking on False Positive rate</h1>

<p>Below we use simulation to demonstrate the degree to which various data peeking strategies can inflate the False Positive rate of the statistical procedure, and compare those False Positive rates to a valid, no-peeking analysis strategy.</p>

<p>In each of 5,000 simulations below we generate a scenario where our data are sampled from a distribution that has zero mean–in this case a standard Normal. We then use a <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">one-sample t-test</a> to infer whether or not the mean of those samples differs signficantly from zero (the Null hypothesis). Since the samples do in fact have zero mean, we intuitively would expect to <em>always</em> detect no difference from zero. However, due to randomness in the sampling process, we’ll obtain a difference by chance some small percentage of the time.</p>

<p>Additionally, for each simulation we’ll “peek” at the partial dataset at various points during the data collection, based one of the following peeking strategies:</p>

<ul>
  <li>Peeking after every sample (worst case scenario)</li>
  <li>Peek intermittently, every 10th, 20th, or 50th sample collected</li>
  <li>No peeking</li>
</ul>

<p>After each peek at the current dataset, we then decide whether to keep collecting data contingent on what we’ve observed so far:</p>

<ul>
  <li>If we detect a statistically-significant difference based on \(\alpha\) and the t-test applied to the partial data set, then we stop data collection.</li>
  <li>Otherwise, we continue to collect data until the next peeking observation.</li>
</ul>

<p>Given that we know the ground truth distribution, we can calculate how much the False Positive rate has increased for each peeking strategy over the course of data collection.</p>

<p>The results of the simulation are plotted in <strong><em>Figure 1</em></strong>.</p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="600" src="assets/images/p-hacking-data-peeking/peeking-every.png" />
    </div>
</center>

<p><strong><em>Figure 1, False positive (Type I) error rates associated with various data-peeking strategies.</em></strong> <em>As we increase the frequency of data peeking, we increase the False Positive rate of our inference procedure. This inflation increases with the number peeks and samples collected. If we were to peak at every sample, we would have a nearly 1 in 2 chance of a False Positive after collecting ~500 samples. No data peeking closely follows the researcher-defined accptable False Positive rate for the experiment \(\alpha=0.05\).</em></p>

<details>
  <summary>Python Code to generate <b><i>Figure 1</i></b></summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">vis</span> <span class="kn">import</span> <span class="n">COLORS</span><span class="p">,</span> <span class="n">save_figure</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="s">"""
Simulate 5000 experiments with 500 samples each. There is no statistical
difference in the mean of the sampled data from zero (our Null Hypothesis)
"""</span>
<span class="n">n_samples_per_simulation</span><span class="p">,</span> <span class="n">n_simulations</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">5000</span>
<span class="n">real_mean</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">sampling_distribution</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">real_mean</span><span class="p">)</span>
<span class="n">simulations</span> <span class="o">=</span> <span class="n">sampling_distribution</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span>
    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples_per_simulation</span><span class="p">,</span> <span class="n">n_simulations</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Simulate many hypothesis tests, increasing the sample size for each
</span><span class="n">p_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">min_samples</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Minimum samples used to run a t-test
</span><span class="k">for</span> <span class="n">n_samples_used</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_samples</span><span class="p">,</span> <span class="n">n_samples_per_simulation</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_samples_used</span><span class="p">)</span>

    <span class="c1"># p-values obtained from one-sample t-test
</span>    <span class="n">p_values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">stats</span><span class="p">.</span><span class="n">ttest_1samp</span><span class="p">(</span><span class="n">simulations</span><span class="p">[:</span><span class="n">n_samples_used</span><span class="p">],</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">pvalue</span>
    <span class="p">)</span>

<span class="c1"># Make null-hypothesis decisions based on p-values
</span><span class="n">alpha</span> <span class="o">=</span> <span class="p">.</span><span class="mi">05</span>  <span class="c1"># Researcher-defined acceptable Type I error rate
</span><span class="n">decisions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">alpha</span>

<span class="s">"""
We simulate peeking by denoting all observations that follow
observing an initial positive result to also be considered positive.
This is equivalent to not collecting any more data.
"""</span>
<span class="c1"># Simulate peeking every N-th sample strategy
</span><span class="n">peeking_strategy</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">peeking_strategy_colors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"black"</span><span class="p">,</span>
    <span class="n">COLORS</span><span class="p">.</span><span class="n">dark_red</span><span class="p">,</span>
    <span class="n">COLORS</span><span class="p">.</span><span class="n">red</span><span class="p">,</span>
    <span class="n">COLORS</span><span class="p">.</span><span class="n">salmon</span>
<span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ci</span><span class="p">,</span> <span class="n">peek_every_nth</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">peeking_strategy</span><span class="p">):</span>

    <span class="n">intermitten_peeking_decisions</span> <span class="o">=</span> <span class="n">decisions</span><span class="p">[::</span><span class="n">peek_every_nth</span><span class="p">].</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;=</span><span class="mi">1</span>
    <span class="n">intermitten_peeking_type_I_error_rate</span> <span class="o">=</span> <span class="n">intermitten_peeking_decisions</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Add plot for this strategy
</span>    <span class="n">label</span> <span class="o">=</span> <span class="s">'Every Sample'</span> <span class="k">if</span> <span class="n">peek_every_nth</span> <span class="o">==</span> <span class="mi">1</span> \
        <span class="k">else</span> <span class="sa">f</span><span class="s">'Every </span><span class="si">{</span><span class="n">peek_every_nth</span><span class="si">}</span><span class="s">th Sample'</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">n_samples</span><span class="p">[::</span><span class="n">peek_every_nth</span><span class="p">],</span>
        <span class="n">intermitten_peeking_type_I_error_rate</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">peeking_strategy_colors</span><span class="p">[</span><span class="n">ci</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="n">label</span>
    <span class="p">)</span>

<span class="s">"""
We simulate no peaking as just the average Type I error
rate across all simulations without any results-dependent
screening.
"""</span>
<span class="n">type_I_error_rate</span> <span class="o">=</span> <span class="n">decisions</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">type_I_error_rate</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'No Peeking'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span>
    <span class="n">alpha</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="n">COLORS</span><span class="p">.</span><span class="n">gray</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> 
    <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$\alpha={:0.2f}$'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">min_samples</span><span class="p">,</span> <span class="n">n_samples_per_simulation</span> <span class="o">-</span> <span class="n">peeking_strategy</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of Samples"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Type I Error Rate"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span>
    <span class="s">"The effect of peeking at your AB test</span><span class="se">\n</span><span class="s">"</span>
    <span class="s">"results before data collection is complete"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"Peeking Strategy"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">();</span>
</code></pre></div>  </div>
</details>

<p><br /></p>

<hr />
<p><br /></p>

<p>We can see that the more often we peek at our data, the larger our False Positive rate becomes over the course of the experiment’s data collection. In the worst case scenario, where we peek at every sample, our False Positive rate is nearly 50% after collecting ~500 samples!</p>

<p>We can also see how <em>not peeking</em> allows us to maintain the guarantees of the statistical test on False Positive rate. When we don’t peek at our data, our expected Type I error rate is consistent with the \(\alpha\) used in the t-test.</p>

<h1 id="wrapping-up">Wrapping Up</h1>

<p>Data-peeking is just one of many spooky gotchas that come along with using Null hypothesis-based statistical tests (NHST). This particular <em>p</em>-hacking effect comes up when we prematurely run statistical tests on our experiment data before the required dataset size has been reached. If you’ve ever said something like “let’s just take a look at the experimet to see if we’ve reached statsig,” then you’re probably p-hacking!</p>

<p>The negative affects of data-peeking can be minimized by sticking to standardized protocols for running experiments that use NHSTs: running an initial <a href="https://en.wikipedia.org/wiki/Sample_size_determination">power analysis</a> to calculate the required sample size for a desired <a href="https://en.wikipedia.org/wiki/Effect_size">effect size</a> and <a href="https://en.wikipedia.org/wiki/Power_of_a_test">statistical power</a>, then holding off your statistical analysis till the proper sample size has been reached.</p>

<hr />
<hr />

  </div><div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'the-clever-machine'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/theclevermachine/p-hacking-data-peeking" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
