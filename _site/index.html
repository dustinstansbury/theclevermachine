<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="stylesheet" href="/theclevermachine/assets/main.css"">
  <link rel=" icon" type="image/png" href="icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Clever Machine | Musings on data and science</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="The Clever Machine" />
<meta name="author" content="Dustin Stansbury, PhD" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Musings on data and science" />
<meta property="og:description" content="Musings on data and science" />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/" />
<meta property="og:site_name" content="The Clever Machine" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Clever Machine" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Dustin Stansbury, PhD"},"description":"Musings on data and science","headline":"The Clever Machine","name":"The Clever Machine","url":"https://dustinstansbury.github.io/theclevermachine/"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a><a class="page-link" href="/theclevermachine/topics/">Topics</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
<h2 class="post-list-heading">Posts</h2>
  <ul class="post-list"><li>
      <h3>
        <a class="post-link" href="/theclevermachine/p-hacking-data-peeking">
          p-Hacking 101: Data Peeking
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Dec 13, 2023 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/statistics.html">statistics</a>
          
          
          <a href="/theclevermachine/topics/hypothesis-testing.html">hypothesis-testing</a>
          
          
          <a href="/theclevermachine/topics/ab-testing.html">ab-testing</a>
          
          
          <a href="/theclevermachine/topics/false-positive.html">false-positive</a>
          
          
          <a href="/theclevermachine/topics/type-I-error.html">type-I-error</a>
          
          
          <a href="/theclevermachine/topics/p-hacking.html">p-hacking</a>
          
          
          
        </span>
      </h3><p><em>“Data peeking”</em> is the process of prematurely running statistical tests on your AB experiment data before data collection has reached the required sample size prescribed by <a href="https://en.wikipedia.org/wiki/Sample_size_determination">power analysis</a>. You may have heard of the dangers of data peeking, but may not have an intuition as to how dramatically it can inflate your False Positive rate, and thus mislead statistical inferences. In this post we’ll use simulations to demonstrate just how much data peeking can inflate false positives.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/p-hacking-n-chasing">
          p-Hacking 101: N Chasing
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Oct 4, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/statistics.html">statistics</a>
          
          
          <a href="/theclevermachine/topics/hypothesis-testing.html">hypothesis-testing</a>
          
          
          <a href="/theclevermachine/topics/ab-testing.html">ab-testing</a>
          
          
          <a href="/theclevermachine/topics/false-positive.html">false-positive</a>
          
          
          <a href="/theclevermachine/topics/type-I-error.html">type-I-error</a>
          
          
          <a href="/theclevermachine/topics/p-hacking.html">p-hacking</a>
          
          
          
        </span>
      </h3><p>”\(N\) Chasing,” or adding new observations to an already-analyzed experiment can increase your experiment’s false positive rate. As an experimenter or analyst, you may have heard of the dangers of \(N\) chasing, but may not have an intuition as to why or how it increases Type I Error. In this post we’ll demonstrate \(N\) chasing using some simulations, and show that, under certain settings, adding just a single data point to your experiment can dramatically increase false positives.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/info-theory-word-embeddings">
          Who Needs Backpropagation? Computing Word Embeddings with Linear Algebra
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Sep 11, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/natural-language-processing.html">natural-language-processing</a>
          
          
          <a href="/theclevermachine/topics/word-embeddings.html">word-embeddings</a>
          
          
          <a href="/theclevermachine/topics/information-theory.html">information-theory</a>
          
          
          <a href="/theclevermachine/topics/pointwise-mutual-information.html">pointwise-mutual-information</a>
          
          
          <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
          
          
          <a href="/theclevermachine/topics/singular-value-decomposition.html">singular-value-decomposition</a>
          
          
          
        </span>
      </h3><p>Word embeddings provide numerical representations of words that carry useful semantic information about natural language. This has made word embeddings an integral part of modern Natural Language Processing (NLP) pipelines and language understanding models. Common methods used to compute word embeddings, like <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>, employ predictive, neural network frameworks. However, as we’ll show in this post, we can also compute word embeddings using a some basic frequency statistics, a little information theory, and our good old friend from linear algebra, <a href="/theclevermachine/singular-value-decomposition">Singular Value Decomposition</a>.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/svd-data-compression">
          SVD and Data Compression Using Low-rank Matrix Approximation
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Aug 16, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
          
          
          <a href="/theclevermachine/topics/singular-value-decomposition.html">singular-value-decomposition</a>
          
          
          <a href="/theclevermachine/topics/low-rank-approximation.html">low-rank-approximation</a>
          
          
          <a href="/theclevermachine/topics/data-compression.html">data-compression</a>
          
          
          <a href="/theclevermachine/topics/image-compression.html">image-compression</a>
          
          
          
        </span>
      </h3><p>In a <a href="/theclevermachine/singular-value-decomposition">previous post</a> we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/singular-value-decomposition">
          Singular Value Decomposition: The Swiss Army Knife of Linear Algebra
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Aug 10, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
          
          
          <a href="/theclevermachine/topics/matrix-diagonalization.html">matrix-diagonalization</a>
          
          
          <a href="/theclevermachine/topics/singular-value-decomposition.html">singular-value-decomposition</a>
          
          
          
        </span>
      </h3><p>Linear algebra provides a number powerful computational methods that are used throughout the sciences. However, I would say that hands-down the most versatile of these methods is singular value decomposition, or SVD. In this post we’ll dive into a little theory behind matrix diagonalization and show how SVD generalizes matrix diagonalization. Then we’ll go into a few of the properties of SVD and cover a few (of many!) cool and useful applications of SVD in the real world. In addition, each application will have its own dedicated post.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/matrix-power-using-diagonalization">
          Efficient Matrix Power Calculation via Diagonalization
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Aug 8, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
          
          
          <a href="/theclevermachine/topics/matrix-diagonalization.html">matrix-diagonalization</a>
          
          
          
        </span>
      </h3><p>Taking the power of a matrix is an important operation with applications in statistics, machine learning, and engineering. For example, solving linear ordinary differential equations, identifying the state of a Markov chain at time \(t\), or identifying the number of paths between nodes in a graph can all be solved using powers of matrices. In this quick post we’ll show how Matrix Diagonalization can be used to efficiently compute the power of a matrix.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/linear-algebra-identities">
          Common Linear Algebra Identities
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Aug 5, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/derivation.html">derivation</a>
          
          
          <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
          
          
          <a href="/theclevermachine/topics/matrix-identities.html">matrix-identities</a>
          
          
          
        </span>
      </h3><p>This post provides a convenient reference of Linear Algebra identities used in The Clever Machine Blog.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/derivation-normal-equations">
          Derivation: Ordinary Least Squares Solution and the Normal Equations
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jul 23, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/ordinary-least-squares.html">ordinary-least-squares</a>
          
          
          <a href="/theclevermachine/topics/derivation.html">derivation</a>
          
          
          <a href="/theclevermachine/topics/normal-equations.html">normal-equations</a>
          
          
          
        </span>
      </h3><p>Have you ever performed linear regression involving multiple predictor variables and run into this expression \(\hat \beta = (X^TX)^{-1}X^Ty\)? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/bias-variance-tradeoff">
          Model Selection: Underfitting, Overfitting, and the Bias-Variance Tradeoff
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jul 20, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/statistics.html">statistics</a>
          
          
          <a href="/theclevermachine/topics/classification.html">classification</a>
          
          
          <a href="/theclevermachine/topics/regression.html">regression</a>
          
          
          <a href="/theclevermachine/topics/bias-variance-tradeoff.html">bias-variance-tradeoff</a>
          
          
          <a href="/theclevermachine/topics/model-selection.html">model-selection</a>
          
          
          
        </span>
      </h3><p>In machine learning and pattern recognition, there are many ways (an infinite number, really) of solving any one problem. Thus it is important to have an objective criterion for assessing the accuracy of candidate approaches and for selecting the right model for a data set at hand. In this post we’ll discuss the concepts of under- and overfitting and how these phenomena are related to the statistical quantities bias and variance. Finally, we will discuss how these concepts can be applied to select a model that will accurately generalize to novel scenarios/data sets.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/supplemental-lemma-expectation-x-squared">
          Supplemental Proof: The Expected Value of a Squared Random Variable
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jul 19, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/statistics.html">statistics</a>
          
          
          <a href="/theclevermachine/topics/derivation.html">derivation</a>
          
          
          <a href="/theclevermachine/topics/expected-value.html">expected-value</a>
          
          
          
        </span>
      </h3><p>We want to show the following relationship:</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/a-gentle-introduction-to-neural-networks">
          A Gentle Introduction to Artificial Neural Networks
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jul 13, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/neural-networks.html">neural-networks</a>
          
          
          <a href="/theclevermachine/topics/gradient-descent.html">gradient-descent</a>
          
          
          <a href="/theclevermachine/topics/backpropagation.html">backpropagation</a>
          
          
          <a href="/theclevermachine/topics/classification.html">classification</a>
          
          
          <a href="/theclevermachine/topics/regression.html">regression</a>
          
          
          <a href="/theclevermachine/topics/deep-learning.html">deep-learning</a>
          
          
          
        </span>
      </h3><p>Though many phenomena in the world can be well-modeled using basic linear regression or classification, there are also many interesting phenomena that are nonlinear in nature. In order to deal with nonlinear phenomena, there have been a diversity of nonlinear models developed.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/cutting-your-losses">
          Cutting Your Losses: Loss Functions &amp; the Sum of Squared Errors Loss
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jun 30, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/statistics.html">statistics</a>
          
          
          <a href="/theclevermachine/topics/least-squares-regression.html">least-squares-regression</a>
          
          
          <a href="/theclevermachine/topics/loss-functions.html">loss-functions</a>
          
          
          <a href="/theclevermachine/topics/parameter-optimization.html">parameter-optimization</a>
          
          
          <a href="/theclevermachine/topics/r-squared.html">r-squared</a>
          
          
          
        </span>
      </h3><p>In this post we’ll introduce the notion of the loss function and its role in model parameter estimation. We’ll then focus in on a common loss function–the sum of squared errors (SSE) loss–and give some motivations and intuitions as to why this particular loss function works so well in practice.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/derivation-common-neural-network-activation-functions">
          Derivation: Derivatives for Common Neural Network Activation Functions
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jun 29, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/neural-networks.html">neural-networks</a>
          
          
          <a href="/theclevermachine/topics/gradient-descent.html">gradient-descent</a>
          
          
          <a href="/theclevermachine/topics/derivation.html">derivation</a>
          
          
          
        </span>
      </h3><p>When constructing Artificial Neural Network (ANN) models, one of the primary considerations is choosing activation functions for hidden and output layers that are differentiable. This is because calculating the backpropagated error signal that is used to determine ANN parameter updates requires the gradient of the activation function gradient . Three of the most commonly-used activation functions used in ANNs are the identity function, the logistic sigmoid function, and the hyperbolic tangent function. Examples of these functions and their associated gradients (derivatives in 1D) are plotted in Figure 1.</p>

</li><li>
      <h3>
        <a class="post-link" href="/theclevermachine/derivation-backpropagation">
          Derivation: Error Backpropagation &amp; Gradient Descent for Neural Networks
        </a><span class="post-meta"> <i class="fa-solid fa-calendar"></i> Jun 29, 2020 <i
            class="fa-solid fa-pencil"></i> Dustin Stansbury
          <br> <i class="fa-solid fa-tags"></i>
          
          
          
          <a href="/theclevermachine/topics/neural-networks.html">neural-networks</a>
          
          
          <a href="/theclevermachine/topics/gradient-descent.html">gradient-descent</a>
          
          
          <a href="/theclevermachine/topics/derivation.html">derivation</a>
          
          
          
        </span>
      </h3><p>Artificial neural networks (ANNs) are a powerful class of models used for nonlinear regression and classification tasks that are motivated by biological neural computation. The general idea behind ANNs is pretty straightforward: map some input onto a desired target value using a distributed cascade of nonlinear transformations (see <strong><em>Figure 1</em></strong>). However, for many, myself included, the learning algorithm used to train ANNs can be difficult to get your head around at first. In this post I give a step-by-step walkthrough of the derivation of the gradient descent algorithm commonly used to train ANNs–aka the “backpropagation” algorithm. Along the way, I’ll also try to provide some high-level insights into the computations being performed during learning<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Though, I guess these days with autograd, who <em>really</em> needs to understand how the calculus for gradient descent works, amiright? (<em>hint</em>: that is a joke) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
</li></ul>

  <p class="rss-subscribe">subscribe <a href="/theclevermachine/%20/feed.xml">via RSS</a></p></div>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
