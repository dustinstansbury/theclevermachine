<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <link rel="stylesheet" href="/theclevermachine/assets/main.css"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>The Clever Machine | Musings on data and science</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="The Clever Machine" />
<meta name="author" content="Dustin Stansbury, PhD" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Musings on data and science" />
<meta property="og:description" content="Musings on data and science" />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/" />
<meta property="og:site_name" content="The Clever Machine" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Dustin Stansbury, PhD"},"url":"https://dustinstansbury.github.io/theclevermachine/","description":"Musings on data and science","@type":"WebSite","headline":"The Clever Machine","name":"The Clever Machine","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
<h2 class="post-list-heading">Posts</h2>
    <ul class="post-list"><li><span class="post-meta">Aug 16, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/svd-data-compression">
            SVD and Data Compression Using Low-rank Matrix Approximation
          </a>
        </h3><p>In a <a href="/theclevermachine/singular-value-decomposition">previous post</a> we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA.</p>

</li><li><span class="post-meta">Aug 10, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/singular-value-decomposition">
            Singular Value Decomposition: The Swiss Army Knife of Linear Algebra
          </a>
        </h3><p>Linear algebra provides a number powerful computational methods that are used throughout the sciences. However, I would say that hands-down the most versatile of these methods is singular value decomposition, or SVD. In this post we’ll dive into a little theory behind matrix diagonalization and show how SVD generalizes matrix diagonalization. Then we’ll go into a few of the properties of SVD and cover a few (of many!) cool and useful applications of SVD in the real world. In addition, each application will have its own dedicated post.</p>

</li><li><span class="post-meta">Aug 8, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/matrix-power-using-diagonalization">
            Efficient Matrix Power Calculation via Diagonalization
          </a>
        </h3><p>Taking the power of a matrix is an important operation with applications in statistics, machine learning, and engineering. For example, solving linear ordinary differential equations, identifying the state of a Markov chain at time <script type="math/tex">t</script>, or identifying the number of paths between nodes in a graph can all be solved using powers of matrices. In this quick post we’ll show how Matrix Diagonalization can be used to efficiently compute the power of a matrix.</p>

</li><li><span class="post-meta">Aug 5, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/linear-algebra-identities">
            Common Linear Algebra Identities
          </a>
        </h3><p>This post provides a convenient reference of Linear Algebra identities used in The Clever Machine Blog.</p>

</li><li><span class="post-meta">Jul 23, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/derivation-normal-equations">
            Derivation: Ordinary Least Squares Solution and the Normal Equations
          </a>
        </h3><p>Have you ever performed linear regression involving multiple predictor variables and run into this expression <script type="math/tex">\hat \beta = (X^TX)^{-1}X^Ty</script>? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!</p>

</li><li><span class="post-meta">Jul 20, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/bias-variance-tradeoff">
            Model Selection: Underfitting, Overfitting, and the Bias-Variance Tradeoff
          </a>
        </h3><p>In machine learning and pattern recognition, there are many ways (an infinite number, really) of solving any one problem. Thus it is important to have an objective criterion for assessing the accuracy of candidate approaches and for selecting the right model for a data set at hand. In this post we’ll discuss the concepts of under- and overfitting and how these phenomena are related to the statistical quantities bias and variance. Finally, we will discuss how these concepts can be applied to select a model that will accurately generalize to novel scenarios/data sets.</p>

</li><li><span class="post-meta">Jul 19, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/supplemental-lemma-expectation-x-squared">
            Supplemental Proof: The Expected Value of a Squared Random Variable
          </a>
        </h3><p>We want to show the following relationship:</p>

</li><li><span class="post-meta">Jul 13, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/a-gentle-introduction-to-neural-networks">
            A Gentle Introduction to Artificial Neural Networks
          </a>
        </h3><p>Though many phenomena in the world can be well-modeled using basic linear regression or classification, there are also many interesting phenomena that are nonlinear in nature. In order to deal with nonlinear phenomena, there have been a diversity of nonlinear models developed.</p>

</li><li><span class="post-meta">Jun 30, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/cutting-your-losses">
            Cutting Your Losses: Loss Functions &amp; the Sum of Squared Errors Loss
          </a>
        </h3><p>In this post we’ll introduce the notion of the loss function and its role in model parameter estimation. We’ll then focus in on a common loss function–the sum of squared errors (SSE) loss–and give some motivations and intuitions as to why this particular loss function works so well in practice.</p>

</li><li><span class="post-meta">Jun 29, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/derivation-common-neural-network-activation-functions">
            Derivation: Derivatives for Common Neural Network Activation Functions
          </a>
        </h3><p>When constructing Artificial Neural Network (ANN) models, one of the primary considerations is choosing activation functions for hidden and output layers that are differentiable. This is because calculating the backpropagated error signal that is used to determine ANN parameter updates requires the gradient of the activation function gradient . Three of the most commonly-used activation functions used in ANNs are the identity function, the logistic sigmoid function, and the hyperbolic tangent function. Examples of these functions and their associated gradients (derivatives in 1D) are plotted in Figure 1.</p>

</li><li><span class="post-meta">Jun 29, 2020</span>
        <h3>
          <a class="post-link" href="/theclevermachine/derivation-backpropagation">
            Derivation: Error Backpropagation &amp; Gradient Descent for Neural Networks
          </a>
        </h3><p>Artificial neural networks (ANNs) are a powerful class of models used for nonlinear regression and classification tasks that are motivated by biological neural computation. The general idea behind ANNs is pretty straightforward: map some input onto a desired target value using a distributed cascade of nonlinear transformations (see <strong><em>Figure 1</em></strong>). However, for many, myself included, the learning algorithm used to train ANNs can be difficult to get your head around at first. In this post I give a step-by-step walkthrough of the derivation of the gradient descent algorithm commonly used to train ANNs–aka the “backpropagation” algorithm. Along the way, I’ll also try to provide some high-level insights into the computations being performed during learning<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Though, I guess these days with autograd, who <em>really</em> needs to understand how the calculus for gradient descent works, amiright? (<em>hint</em>: that is a joke) <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</li></ul>

    <p class="rss-subscribe">subscribe <a href="/theclevermachine/feed.xml">via RSS</a></p></div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
