<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://dustinstansbury.github.io/theclevermachine/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dustinstansbury.github.io/theclevermachine/" rel="alternate" type="text/html" /><updated>2020-07-07T15:50:14-07:00</updated><id>https://dustinstansbury.github.io/theclevermachine/feed.xml</id><title type="html">The Clever Machine</title><subtitle>Musings on data and science</subtitle><author><name>Dustin Stansbury, PhD</name></author><entry><title type="html">Cutting Your Losses: Loss Functions &amp;amp; the Sum of Squared Errors Loss</title><link href="https://dustinstansbury.github.io/theclevermachine/cutting-your-losses" rel="alternate" type="text/html" title="Cutting Your Losses: Loss Functions &amp; the Sum of Squared Errors Loss" /><published>2020-06-30T00:00:00-07:00</published><updated>2020-06-30T00:00:00-07:00</updated><id>https://dustinstansbury.github.io/theclevermachine/cutting-your-losses</id><content type="html" xml:base="https://dustinstansbury.github.io/theclevermachine/cutting-your-losses">&lt;p&gt;In this post we’ll introduce the notion of the loss function and its role in model parameter estimation. We’ll then focus in on a common loss function–the sum of squared errors (SSE) loss–and give some motivations and intuitions as to why this particular loss function works so well in practice.&lt;/p&gt;

&lt;h1 id=&quot;model-estimation-and-loss-functions&quot;&gt;Model Estimation and Loss Functions&lt;/h1&gt;

&lt;p&gt;Often times, particularly in a regression framework, we are given a set of inputs (independent variables) &lt;script type=&quot;math/tex&quot;&gt;\bf{x}&lt;/script&gt; and a set outputs (dependent variables) &lt;script type=&quot;math/tex&quot;&gt;\bf{y}&lt;/script&gt;, and we want to devise a model function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\mathbf{x})=\mathbf{y} \tag{1}&lt;/script&gt;

&lt;p&gt;that predicts the outputs given some inputs as best as possible. By “devise a model,” we generally mean estimating the parameter values of a particular model form (e.g. the weights of each term in a polynomial model, the layer weights in a neural network, etc.). But what does it mean for a model to predict “as best as possible” exactly? In order to make the notion of how good a model is explicit, it is common to adopt a loss function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(f(\mathbf{x});\mathbf{y}) \tag{2}&lt;/script&gt;

&lt;p&gt;The loss function takes &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; input-output pairs &lt;script type=&quot;math/tex&quot;&gt;(x_i, y_i), i=1..M&lt;/script&gt; and returns a scalar value indicating the overall error or “loss” that results from using the model function &lt;script type=&quot;math/tex&quot;&gt;f()&lt;/script&gt; to predict the outputs &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; from the associated inputs &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;. “Good” models will result in a small loss values. Determining the “best” model is equivalent to finding model function that minimizes the loss function.&lt;/p&gt;

&lt;p&gt;We generally don’t want tweak model parameters by hand, so we rely on computer programs to do the tweaking for us. There are lots of ways we can program a computer to use a loss function to identify good parameters–some examples include &lt;a href=&quot;https://en.wikipedia.org/wiki/Genetic_algorithm&quot;&gt;genetic algorithms&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cuckoo_search&quot;&gt;cuckoo search&lt;/a&gt;, and the &lt;a href=&quot;https://en.wikipedia.org/wiki/MCS_algorithm&quot;&gt;MCS algorithm&lt;/a&gt;. However, by far the most common class of parameter optimization methods are &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;gradient descent (GD) methods&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We won’t go deeply into GD in this post, but will point out that using GD is a process of finding optimal parameters by treating the loss function as a surface in the space of parameters, then following this surface “downhill” to the nearest location in parameter space that minimizes the loss. In order to determine the direction of “downhill”, the loss function generally needs to be differentiable at all the values that the parameters can take.&lt;/p&gt;

&lt;h1 id=&quot;the-sum-of-squared-errors-loss&quot;&gt;The Sum of Squared Errors Loss&lt;/h1&gt;

&lt;p&gt;Arguably, the most common loss function used in statistics and machine learning is the &lt;em&gt;sum of squared of the errors (SSE)&lt;/em&gt; loss function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}

J_{SSE}(f(\mathbf{x});\mathbf{y}) &amp;= \sum_{i=1}^M (y_i - f(x_i))^2 \\
&amp;= \sum_{i=1}^M e_i^2  \tag{3} 

\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This formula for &lt;script type=&quot;math/tex&quot;&gt;J_{SSE}&lt;/script&gt; states that for each output predicted by the model &lt;script type=&quot;math/tex&quot;&gt;f(x_i)&lt;/script&gt;, we determine how “far away” the prediction is from the actual value &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;. Distance is quantified by first taking the difference between the two values and squaring it. The difference between the predicted and actual value is often referred to as the model “error” or “residual” &lt;script type=&quot;math/tex&quot;&gt;e_i&lt;/script&gt; for the datapoint. The semantics here being that small errors correspond to small distances. Each of the &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; distances are then aggregated across the entire dataset through addition, giving a single number indicating how well (or badly) the current model function captures the structure of the entire dataset. The “best” model will minimize the SSE and is called the &lt;em&gt;least sum of squares (LSS)&lt;/em&gt; solution.&lt;/p&gt;

&lt;p&gt;But why use this particular form of loss function in &lt;strong&gt;&lt;em&gt;Equation 3&lt;/em&gt;&lt;/strong&gt;? Why &lt;em&gt;square&lt;/em&gt; the errors before summing them? At first, these decisions seems somewhat arbitrary. Surely there are other, more straight-forward loss functions we can devise. An initial notion of just adding the errors leads to a dead end because adding many positive and negative errors (i.e. resulting from data located below and above the model function) just cancels out; we want our measure of errors to be cumulative. Another idea would be to just take the absolute value of the errors &lt;script type=&quot;math/tex&quot;&gt;\mid e_i \mid&lt;/script&gt; before summing. It turns out, this is also a common loss function, called the &lt;em&gt;sum of absolute errors (SAE)&lt;/em&gt; or &lt;em&gt;sum of absolute deviations (SAD)&lt;/em&gt; loss function. Though SAE/SAD is used regularly for parameter estimation, the SSE loss is generally more popular. So, why does SSE make the cut so often? It turns out there are number of interesting theoretical and practical motivations for using the SSE loss over many other losses. In the remainder of the post we’ll dig into a few of these motivations.&lt;/p&gt;

&lt;h1 id=&quot;geometric-interpretation-and-linear-regression&quot;&gt;Geometric Interpretation and Linear Regression&lt;/h1&gt;

&lt;p&gt;One of the reasons that the SSE loss is used so often for parameter estimation is its close relationship to the formulation of one of the pillars of statistical modeling, &lt;em&gt;linear regression&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt; plots a set of 2-dimensional data (blue circles). In this case the &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; encode horizontal locations and &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; the vertical locations of the (&lt;script type=&quot;math/tex&quot;&gt;M=10&lt;/script&gt;) points in a 2-dimensional (2D) space. A linear function (black line) of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{lin}(\mathbf x) = \beta_0 + \beta_1 \mathbf x \tag{4}&lt;/script&gt;

&lt;p&gt;has been fit to the data. The model parameters &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; (the slope of the line) and &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; (the offset of the line from &lt;script type=&quot;math/tex&quot;&gt;y=0&lt;/script&gt;) have been optimized by &lt;a href=&quot;/theclevermachine/derivation-ols-normal-equations&quot;&gt;minimizing the SSE loss&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;400&quot; src=&quot;assets/images/cutting-your-losses/linear-regression-lss-solution.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;2-dimensional dataset with a Linear Regression model fit. Parameters are fit by minimizing the SSE loss function&lt;/em&gt;&lt;/p&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;patches&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generate Dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;123&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Find LSS solution
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correlation_coefficient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linregress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Plotting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_COLOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MODEL_COLOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'black'&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;standardize_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'square'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot Data and LSS Model Fit
# fig, axs = plt.subplots(3, 1,  figsize=(20, 20))
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# plt.sca(axs[0])
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LSS Model'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;standardize_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Linear Regression Model Fit to Data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A helpful interpretation of the SSE loss function is demonstrated in &lt;strong&gt;&lt;em&gt;Figure 2&lt;/em&gt;&lt;/strong&gt;. The area of each red square is a &lt;em&gt;literal&lt;/em&gt; geometric interpretation of each observation’s contribution to the overall loss. We see that no matter if the errors are positive or negative (i.e. actual &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; are located above or below the black line), the contribution to the loss is always an area, and therefore positive. In this interpretation, &lt;strong&gt;the goal of finding the LSS solution is equivalent to finding the parameters of the line that results in the smallest total red area.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;400&quot; src=&quot;assets/images/cutting-your-losses/lss-gives-rss.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 2&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;The SEE loss that results from the Least Squares Solution (total red area) gives the RSS; or the minimum amount of variance that cannot be explained by a linear model&lt;/em&gt;&lt;/p&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lss_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RSS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plotting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LINEAR_MODEL_ERROR_COLOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_prediction_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error_sign&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error_sign&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bottom_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error_sign&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bottom_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# error squared patch
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;patches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bottom_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;facecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Add the patch to the Axes
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_patch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot Least Sum of Squares Solution and RSS
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error_patch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_prediction_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LINEAR_MODEL_ERROR_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;RSS: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RSS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;standardize_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_patch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;LSS Model Gives Residual Sum of Squares (RSS)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;relation-to-the-coefficient-of-determination-r2&quot;&gt;Relation to the Coefficient of Determination &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;&lt;/h1&gt;

&lt;p&gt;The geometric interpretation is also useful for understanding the important regression metric known as the coefficient of determination &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;, which is an indicator of how well a linear function (i.e. &lt;strong&gt;&lt;em&gt;Equation 4&lt;/em&gt;&lt;/strong&gt;) models a dataset. First, let’s note that the variance of the model residuals takes following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sigma^2_{e} &amp;= \frac{1}{M} \sum_{i=1}^M (e_i - \mu_e)^2 \\
&amp;= C \sum_{i=1}^M e_i^2 \\
&amp;=C J_{SSE}(f(\mathbf{x}); \mathbf{y}) \tag{5}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we have taken the mean of the residual distribution &lt;script type=&quot;math/tex&quot;&gt;\mu_e&lt;/script&gt; to be zero, as is the case in the &lt;a href=&quot;/theclevermachine/derivation-ols-normal-equations&quot;&gt;Ordinary Least Squares (OLS) formulation&lt;/a&gt;. Therefore, we can think of the SSE loss as the (unscaled) variance of the model errors. Therefore &lt;strong&gt;&lt;em&gt;minimizing the SEE loss is equivalent to minimizing the variance of the model residuals.&lt;/em&gt;&lt;/strong&gt; For this reason, the sum of squares loss is often referred to as the &lt;em&gt;Residual Sum of Squares error (RSS)&lt;/em&gt; for linear models.&lt;/p&gt;

&lt;p&gt;Now, imagine that instead of modeling the data with the full linear model depicted in &lt;strong&gt;&lt;em&gt;Figures 1-2&lt;/em&gt;&lt;/strong&gt;, we instead us a simpler model that has no slope parameter, and only a bias/offset parameter (i.e. &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt;). In this case the simpler model, shown in &lt;strong&gt;&lt;em&gt;Figure 3&lt;/em&gt;&lt;/strong&gt;, only captures the mean value of the data along the y-dimension.&lt;/p&gt;

&lt;hr /&gt;

&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;400&quot; src=&quot;assets/images/cutting-your-losses/bias-only-model-gives-rss.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 3&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;The SSE loss that results from a bias-only model (total green area) gives the TSS, which can be thought of as the (unscaled) variance of the data set’s output variables&lt;/em&gt;&lt;/p&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# offset parameter of LSS solution is the mean of outputs
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bias_only_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;bias_only_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TSS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_only_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plotting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BIAS_ONLY_MODEL_COLOR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'green'&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Bias-only Model Solution gives TSS
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_only_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MODEL_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'LSS'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_only_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error_patch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plot_prediction_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BIAS_ONLY_MODEL_COLOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;TSS: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TSS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;standardize_axis_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_patch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Bias-only Model Gives Total Sum of Squares (TSS)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The total squared error in this model corresponds to the (unscaled) variance of the data set itself, and is often referred to as the the &lt;em&gt;Total Sum of Squares (TSS) error&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The metric &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is defined from the RSS and TSS (total red and green areas, respectively) as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2 = 1 - \frac{RSS}{TSS} \tag{5}&lt;/script&gt;

&lt;p&gt;If the linear model is doing a good job of fitting the data, then the variance of the model errors (RSS, red area) will be small compared to the variance of the dataset (TSS, green area), and the &lt;script type=&quot;math/tex&quot;&gt;R^2 \rightarrow 1&lt;/script&gt;. If the model is doing a poor job of fitting the data, then the variance residuals will approach that of the data itself, and  &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; will be close to zero&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. This is why the &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; metric is often used to describe “the amount of variance in the data accounted for by the linear model.”&lt;/p&gt;

&lt;h1 id=&quot;relation-to-the-correlation-coefficient-pearsons-r&quot;&gt;Relation to the correlation coefficient, Pearson’s &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;&lt;/h1&gt;

&lt;p&gt;During the model fitting using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scipy.stats.linregress&lt;/code&gt;, we made sure to assign the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;correlation_coefficient&lt;/code&gt; variable returned from the optimization procedure. This is &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;&gt;Pearson’s &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; correlation coefficient (CC)&lt;/a&gt;, and is defined as the covariance between two random variables (i.e. two equal-length arrays of numbers) &lt;script type=&quot;math/tex&quot;&gt;A, B&lt;/script&gt;, rescaled by the product of each variable’s standard deviation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r = \frac{\text{Cov}(A, B)}{\sigma_A \sigma_B} \tag{6}&lt;/script&gt;

&lt;p&gt;In the case of linear regression, &lt;script type=&quot;math/tex&quot;&gt;A=\mathbf y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B=f(\mathbb{x})&lt;/script&gt;. In this scenario, it turns out that &lt;script type=&quot;math/tex&quot;&gt;r = \sqrt{R^2}&lt;/script&gt;. Therefore if we calculate &lt;script type=&quot;math/tex&quot;&gt;\sqrt{1 - \frac{RSS}{TSS}}&lt;/script&gt;, we should recover the correlation coefficient. We verify this notion with the results of our simulation in three ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Calculating CC from the &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; derived from the SSE loss function&lt;/li&gt;
  &lt;li&gt;Calculating the CC from raw  data and predictions&lt;/li&gt;
  &lt;li&gt;Verifying the CC returned by the scipy fitting procedure&lt;/li&gt;
&lt;/ol&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lss_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RSS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias_only_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TSS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_only_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r_squared&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RSS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TSS&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_cc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&quot;0. R^2 derived from loss functions: {:1.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_squared&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;1. Correlation derived from R^2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;2. Raw correlation (numpy): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;3. Correlation returned by model fit: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correlation_coefficient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 0. R^2 derived from loss functions: 0.869
# 1. Correlation derived from R^2: 0.932
# 2. Raw correlation (numpy): 0.932
# 3. Correlation returned by model fit: 0.932
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that we’re able to recover the CC from our loss function values RSS and TSS. Therefore we can think of &lt;strong&gt;&lt;em&gt;minimizing the SSE loss as maximizing the covariance between the real outputs and those predicted by the model.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;physical-interpretation--variable-covariance&quot;&gt;Physical Interpretation &amp;amp; Variable Covariance&lt;/h1&gt;

&lt;p&gt;We can gain some additional insight to the importance of minimizing the SSE loss by developing concepts within the framework of a physical system, depicted in &lt;strong&gt;&lt;em&gt;Figure 4&lt;/em&gt;&lt;/strong&gt;. In this formulation, a set of springs (red, dashed lines, our errors &lt;script type=&quot;math/tex&quot;&gt;e_i&lt;/script&gt;) suspend a bar (black line, our linear function &lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{x})&lt;/script&gt;) to a set of anchors (blue datapoints, our outputs &lt;script type=&quot;math/tex&quot;&gt;\mathbf{y}&lt;/script&gt;). Note that in this formulation, the springs are constrained to operate only along the vertical direction (&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;-dimension). This constraint is equivalent to saying that there is only error in our measurement of the dependent variables, and is often an assumption made in regression frameworks.&lt;/p&gt;

&lt;hr /&gt;

&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;400&quot; src=&quot;assets/images/cutting-your-losses/ols-as-physical-system.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 4&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;Modeling linear regression as a physical system of a bar (linear model function &lt;script type=&quot;math/tex&quot;&gt;f(\mathbf x)&lt;/script&gt;) suspended by the force of multiple springs (model errors &lt;script type=&quot;math/tex&quot;&gt;\mathbf e&lt;/script&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;details&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Plot Error as physical system
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plot_data_and_linear_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_springs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ii&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ii&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Spring Force (error)&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
&lt;span class=&quot;n&quot;&gt;plot_springs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Least Squares Linear Regression as a Physical System'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/details&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Hooke%27s_law&quot;&gt;Hooke’s Law&lt;/a&gt;, the force created by each spring on the bar is proportional to the distance (error) from the bar (linear function) to its corresponding anchor (data point):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_i = -ke_i&lt;/script&gt;

&lt;p&gt;Further, there is a potential energy &lt;script type=&quot;math/tex&quot;&gt;U_i&lt;/script&gt; associated with each spring (datapoint). The total potential energy for the system is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sum_i U_i &amp;= \sum_i \int -k e_i \text{d}e_i \\
&amp;= \sum_i \frac{1}{2} k e_i^2 \\ 
&amp;= \sum_i (y_i - f(x_i))^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;(assuming a spring constant of &lt;script type=&quot;math/tex&quot;&gt;k=2&lt;/script&gt;). This demonstrates that the equilibrium state of this system (i.e. the arrangement of the bar that minimizes the potential energy of the system) is analogous to the state that minimizes the sum of the squared error (distance) between the bar (linear function) and the anchors (data points).&lt;/p&gt;

&lt;p&gt;The physical interpretation can also be used to derive how linear regression solutions are related to the variances of the independent variables &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; and the covariance between &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt;. When the bar is in the equilibrium position, the net force  exerted on the bar is zero. Because &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\hat y} = \beta_0 + \beta_1 \mathbf x&lt;/script&gt;, this first zero-net-force condition is formally described as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_i^M y_i - \beta_0 - \beta_1x_i = 0&lt;/script&gt;

&lt;p&gt;A second condition that is fulfilled during equilibrium is that there are no torquing forces on the bar (i.e. the bar is not rotating about an axis). Because &lt;a href=&quot;https://en.wikipedia.org/wiki/Torque&quot;&gt;torque created about an axis&lt;/a&gt; is the force times distance away from the origin (average &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-value; the origin), this second zero-net-torque condition is formally described by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_i^M x_i(y_i - \beta_0 - \beta_1 x_i) = 0&lt;/script&gt;

&lt;p&gt;From the equation corresponding to the first zero-net-force condition&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, we can solve for the bias parameter &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; of the linear function that describes the orientation of the bar:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\beta_0 &amp;=\frac{1}{M}\sum_i (y_i - \beta_1 x_i) \\
&amp;= \bar y - \beta_1 \bar x
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here the &lt;script type=&quot;math/tex&quot;&gt;\bar \cdot&lt;/script&gt; (pronounced “bar”) means the average value. Plugging this expression into the second second zero-net-torque condition equation, we discover that the slope of the line has an interesting interpretation related to the variances of the data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sum_i x_i(y_i - \beta_0 - \beta_1x_i) &amp;= 0 \\

\sum_i x_i(y_i - (\bar y - \beta_1 \bar x) - \beta_1x_i) &amp;= 0 \\

\sum_i x_i(y_i - \bar y) &amp;= \beta_1 \sum_i x_i(x_i - \bar x) \\

\sum_i (x_i - \bar x)(y_i - \bar y) &amp;= \beta_1 \sum_i (x_i -\bar x)^2 \\

\beta_1 = \frac{\sum_i (x_i - \bar x)(y_i - \bar y)}{\sum_i (x_i -\bar x)^2} &amp;= \frac{\text{cov}(x,y)}{\text{var}(x)} 

\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The expressions for the parameters &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; tell us that under least squares linear regression the average of the dependent variables is equal to a scaled version of the average of independent variables plus an offset &lt;script type=&quot;math/tex&quot;&gt;\beta_0&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar y = \beta_0 + \beta_1 \bar x&lt;/script&gt;

&lt;p&gt;Further, the scaling factor &lt;script type=&quot;math/tex&quot;&gt;\beta_1&lt;/script&gt; (the slope) is equal to the ratio of the covariance between the dependent and independent variables to the variance of the independent variable. Therefore if &lt;script type=&quot;math/tex&quot;&gt;\mathbf x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf y&lt;/script&gt; are positively correlated, the slope will be positive, if they are negatively correlated, the slope will be negative.&lt;/p&gt;

&lt;p&gt;Therefore, the SSE loss function directly relates model residuals to how the independent and dependent variables co-vary. These formal covariance-flavored relationships are not available with other loss functions such as the least absolute deviation.&lt;/p&gt;

&lt;p&gt;Because of these relationships, the LSS solution has a number of useful properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The sum of the residuals under the LSS solution is zero (this is equivalent to the zero-net-force condition).&lt;/li&gt;
  &lt;li&gt;Because of 1., the average residual of the LSS solution is zero (this was also pointed out in the geometric interpretation above)&lt;/li&gt;
  &lt;li&gt;The covariance between the independent variables and the residuals is zero (because the &lt;a href=&quot;/theclevermachine/derivation-ols-normal-equations&quot;&gt;residual term in OLS&lt;/a&gt; &lt;script type=&quot;math/tex&quot;&gt;\epsilon \sim N(0, I)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{i.i.d.}&lt;/script&gt;).&lt;/li&gt;
  &lt;li&gt;The LSS solution minimizes the variance of the residuals/model errors (also pointed out in the geometric interpretation).&lt;/li&gt;
  &lt;li&gt;The LSS solution always passes through the mean (center of mass) of the sample.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h1&gt;

&lt;p&gt;Though not covered in this post, there are many other motivations for using the SSE loss over other loss functions, including (but not limited to):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The Least Squares solution can be &lt;a href=&quot;/theclevermachine/derivation-ols-normal-equations&quot;&gt;derived in closed form&lt;/a&gt;, allowing simple analytic implementations and fast computation of model parameters.&lt;/li&gt;
  &lt;li&gt;Unlike the LAE loss, the SSE loss is differentiable (i.e. it is smooth) everywhere, which allows model parameters to be estimated using straight-forward, gradient-based optimizations&lt;/li&gt;
  &lt;li&gt;Squared errors have deep ties in statistics and maximum likelihood estimation methods (as mentioned above), particularly when the errors are distributed according the Normal distribution (as is the case in the OLS formulation)&lt;/li&gt;
  &lt;li&gt;There are a number of geometric and linear algebra theorems that support using least squares. For instance the &lt;a href=&quot;http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem&quot;&gt;Gauss-Markov theorem&lt;/a&gt; states that if errors of a linear function are distributed Normally about the mean of the line, then the LSS solution gives the best &lt;a href=&quot;http://en.wikipedia.org/wiki/Bias_of_an_estimator&quot;&gt;unbiased estimator&lt;/a&gt; for the parameters &lt;script type=&quot;math/tex&quot;&gt;\mathbf \beta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Squared functions have a long history of  facilitating calculus calculations used throughout the physical sciences.
The SSE loss does have a number of downfalls as well. For instance, because each error is squared, any outliers in the dataset can dominate the parameter estimation process. For this reason, the LSS loss is said to lack robustness. Therefore preprocessing of the the dataset (i.e. removing or thresholding outlier values) may be necessary when using the LSS loss.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;notes&quot;&gt;Notes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Note too that the value of  R&lt;script type=&quot;math/tex&quot;&gt;^2&lt;/script&gt; can take negative values, in the case when the RSS is larger than the TSS, indicating a &lt;em&gt;very&lt;/em&gt; poor model. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;What’s interesting, is that the two physical constraint equations derived from the physical system are also obtained through other analytic analyses of linear regression including defining the LSS problem using both &lt;a href=&quot;http://en.wikipedia.org/wiki/Maximum_likelihood&quot;&gt;maximum likelihood estimation&lt;/a&gt; and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Method_of_moments_(statistics)&quot;&gt;method of moments&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Dustin Stansbury</name></author><category term="statistics" /><category term="least-squares-regression" /><category term="loss-functions" /><category term="parameter-optimization" /><category term="r-squared" /><summary type="html">In this post we’ll introduce the notion of the loss function and its role in model parameter estimation. We’ll then focus in on a common loss function–the sum of squared errors (SSE) loss–and give some motivations and intuitions as to why this particular loss function works so well in practice.</summary></entry><entry><title type="html">Derivation: Error Backpropagation &amp;amp; Gradient Descent for Neural Networks</title><link href="https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation" rel="alternate" type="text/html" title="Derivation: Error Backpropagation &amp; Gradient Descent for Neural Networks" /><published>2020-06-29T00:00:00-07:00</published><updated>2020-06-29T00:00:00-07:00</updated><id>https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation</id><content type="html" xml:base="https://dustinstansbury.github.io/theclevermachine/derivation-backpropagation">&lt;p&gt;Artificial neural networks (ANNs) are a powerful class of models used for nonlinear regression and classification tasks that are motivated by biological neural computation. The general idea behind ANNs is pretty straightforward: map some input onto a desired target value using a distributed cascade of nonlinear transformations (see &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;). However, for many, myself included, the learning algorithm used to train ANNs can be difficult to get your head around at first. In this post I give a step-by-step walkthrough of the derivation of the gradient descent algorithm commonly used to train ANNs–aka the “backpropagation” algorithm. Along the way, I’ll also try to provide some high-level insights into the computations being performed during learning&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h1 id=&quot;some-background-and-notation&quot;&gt;Some Background and Notation&lt;/h1&gt;

&lt;p&gt;An ANN consists of an input layer, an output layer, and any number (including zero) of hidden layers situated between the input and output layers. &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt; diagrams an ANN with a single hidden layer. The feed-forward computations performed by the ANN are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The signals from the input layer &lt;script type=&quot;math/tex&quot;&gt;a_i&lt;/script&gt; are multiplied by a set of &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; connecting each input to a node in the hidden layer.&lt;/li&gt;
  &lt;li&gt;These weighted signals are then summed (indicated by &lt;script type=&quot;math/tex&quot;&gt;\sum&lt;/script&gt; in &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;) and combined with a bias &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; (not displayed in &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;). This calculation forms the pre-activation signal &lt;script type=&quot;math/tex&quot;&gt;z_j = b_j + \sum_i a_i w_{ij}&lt;/script&gt; for the hidden layer.&lt;/li&gt;
  &lt;li&gt;The pre-activation signal is then transformed by the hidden layer activation function &lt;script type=&quot;math/tex&quot;&gt;g_j&lt;/script&gt; to form the feed-forward activation signals &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; leaving leaving the hidden layer.&lt;/li&gt;
  &lt;li&gt;In a similar fashion, the hidden layer activation signals &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; are multiplied by the weights connecting the hidden layer to the output layer &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;, summed, and a bias &lt;script type=&quot;math/tex&quot;&gt;b_k&lt;/script&gt; is added.&lt;/li&gt;
  &lt;li&gt;The resulting output layer pre-activation &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; is transformed by the output activation function &lt;script type=&quot;math/tex&quot;&gt;g_k&lt;/script&gt; to form the network output &lt;script type=&quot;math/tex&quot;&gt;a_k&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The computed output &lt;script type=&quot;math/tex&quot;&gt;a_k&lt;/script&gt; is then compared to a desired target value &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; and the error between &lt;script type=&quot;math/tex&quot;&gt;a_k&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; is calculated. This error is used to determine how to update model parameters, as we’ll discuss in the remainder of the post&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;500&quot; src=&quot;assets/images/a-gentle-introduction-to-neural-networks/multi-layer-perceptron.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;: &lt;em&gt;Diagram of an artificial neural network with a single hidden layer (bias units not shown)&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Training a neural network involves determining the set of parameters &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\theta} = \{\mathbf{W},\mathbf{b}\}&lt;/script&gt; that reduces the amount errors that the network makes. Often the choice for the error function is the &lt;a href=&quot;/theclevermachine/cutting-your-losses&quot;&gt;sum of the squared errors&lt;/a&gt; between the target values &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; and the network output &lt;script type=&quot;math/tex&quot;&gt;a_k&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} E &amp;= \frac{1}{2} \sum_{k=1}^K(a_k - t_k)^2 \tag{1} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is the dimensionality of the target/output for a single observation. This parameter optimization problem can be solved using gradient descent, which requires determining &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial E}{\partial \theta}&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in the model.&lt;/p&gt;

&lt;p&gt;Before we begin, let’s define the notation that will be used in remainder of the derivation. Please refer to &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt; for any clarifications.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;{z_j}&lt;/script&gt;: input to node &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;{g_j}&lt;/script&gt;: activation function for node &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; (applied to &lt;script type=&quot;math/tex&quot;&gt;{z_j}&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;a_j=g_j(z_j)&lt;/script&gt;: the output/activation of node &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;{b_{j}}&lt;/script&gt;: bias/offset for unit &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;{w_{ij}}&lt;/script&gt;: weights connecting node &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;(l-1)&lt;/script&gt; to node &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;{t_{k}}&lt;/script&gt;: target value for node &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; in the output layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also note that the parameters for an ANN can be broken up into two distinct sets: those parameters that are associated with the output layer (i.e. &lt;script type=&quot;math/tex&quot;&gt;\theta_k = \{w_{jk}, b_k\}&lt;/script&gt;), and thus directly affect the network output error; and the remaining parameters that are associated with the hidden layer(s), and thus affect the output error indirectly. We’ll first derive the gradients for the output layer parameters, then extend these results to the hidden layer parameters.&lt;/p&gt;

&lt;h1 id=&quot;gradients-for-output-layer-parameters&quot;&gt;Gradients for Output Layer Parameters&lt;/h1&gt;

&lt;h4 id=&quot;output-layer-connection-weights-w_jk&quot;&gt;Output layer connection weights, &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Since the output layer parameters directly affect the value of the error function, determining the gradient of the error function with respect to those parameters is fairly straight-forward using an application of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Chain_rule&quot;&gt;chain rule&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial w_{jk}} &amp;= \frac{1}{2} \sum_{k}(a_k - t_k)^2 \\  
&amp;= (a_k - t_k)\frac{\partial}{\partial w_{jk}}(a_k - t_k) \tag{2}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; is zero because it does not depend on &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;. We can also use the fact that &lt;script type=&quot;math/tex&quot;&gt;a_k = g(z_k)&lt;/script&gt;, and re-apply the chain rule to give&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}\frac{\partial E }{\partial w_{jk}} &amp;= (a_k - t_k)\frac{\partial}{\partial w_{jk}}a_k \\
&amp;= (a_k - t_k)\frac{\partial}{\partial w_{jk}}g_k(z_k) \\
&amp;= (a_k - t_k)g_k'(z_k)\frac{\partial}{\partial w_{jk}}z_k \tag{3}
\end{align} %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, recall that &lt;script type=&quot;math/tex&quot;&gt;z_k = b_k + \sum_j g_j(z_j)w_{jk}&lt;/script&gt; and thus &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_{k}}{\partial w_{jk}} = g_j(z_j) = a_j&lt;/script&gt;, thus giving us:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial E }{\partial w_{jk}} &amp;= \color{red}{(a_k - t_k)}\color{blue}{g_k'(z_k)}\color{green}{a_j} \end{align} \tag{4} %]]&gt;&lt;/script&gt;

&lt;p&gt;From *Equation 4 we can see that the gradient of the error function with respect to the output layer weights &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt; is a product of three terms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{(a_k - t_k)}&lt;/script&gt;: the difference between the network output &lt;script type=&quot;math/tex&quot;&gt;a_k&lt;/script&gt; and the target value &lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{blue}{g_k'(z_k)}&lt;/script&gt;: the derivative of output layer activation function &lt;script type=&quot;math/tex&quot;&gt;g_k()&lt;/script&gt;. For more details on activation function derivatives, please refer to &lt;a href=&quot;/theclevermachine/derivation-common-neural-network-activation-functions&quot;&gt;this post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{green}{a_j}&lt;/script&gt;: the activation signal of node &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; from the hidden layer feeding into the output layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we define &lt;script type=&quot;math/tex&quot;&gt;\delta_k&lt;/script&gt; to be all the terms that involve index &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\color{purple}{\delta_k} = \color{red}{(a_k - t_k)}\color{blue}{g_k'(z_k)} \tag{5}&lt;/script&gt;

&lt;p&gt;Then we get the “delta form” of the error function gradient for the output layer weights:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E }{\partial w_{jk}} = \color{purple}{\delta_k} \color{green}{a_j} \tag{6}&lt;/script&gt;

&lt;p&gt;Here the &lt;script type=&quot;math/tex&quot;&gt;\delta_k&lt;/script&gt; terms can be interpreted as the network output error after being “backpropagated” through the output activation function &lt;script type=&quot;math/tex&quot;&gt;g_k&lt;/script&gt;, thus creating an “error signal”. Loosely speaking, &lt;em&gt;Equation 6&lt;/em&gt; can be interpreted as determining how much each &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt; contributes to the error signal by weighting the error by the magnitude of the output activation from the previous (hidden) layer. The gradients with respect to each &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt; are thus considered to be the “contribution” of that parameter to the total error signal and should be “negated” during learning. This gives the following gradient descent update rule for the output layer weights:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
w_{jk} &amp;\leftarrow w_{jk} - \eta \frac{\partial E }{\partial w_{jk}} \\
&amp;\leftarrow w_{jk} - \eta (\color{purple}{\delta_k} \color{green}{a_j}) \tag{7}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; is some step size, often referred to as the “learning rate”. Similar update rules are used to update the remaining parameters, once &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial E}{\partial \theta}&lt;/script&gt; has been determined.&lt;/p&gt;

&lt;p&gt;As we’ll see shortly, the process of “backpropagating” the error signal can repeated all the way back to the input layer by successively projecting &lt;script type=&quot;math/tex&quot;&gt;\delta_k&lt;/script&gt; back through &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;, then through the activation function &lt;script type=&quot;math/tex&quot;&gt;g'_j(z_j)&lt;/script&gt; for the hidden layer to give the error signal &lt;script type=&quot;math/tex&quot;&gt;\delta_j&lt;/script&gt;, and so on. This backpropagation concept is central to training neural networks with more than one layer.&lt;/p&gt;

&lt;h4 id=&quot;output-layer-biases-b_k&quot;&gt;Output layer biases, &lt;script type=&quot;math/tex&quot;&gt;b_{k}&lt;/script&gt;&lt;/h4&gt;
&lt;p&gt;As for the gradient of the error function with respect to the output layer biases, we follow the same routine as above for &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;. However, the third term in &lt;em&gt;Equation 3&lt;/em&gt; is &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial}{\partial b_k} z_k = \frac{\partial}{\partial b_k} \left[ b_k + \sum_j g_j(z_j)\right] = 1&lt;/script&gt;, giving the following gradient for the output biases:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial b_k} &amp;= (a_k - t_k)g_k'(z_k)(1) \\
&amp;= \color{purple}{\delta_k} \tag{8}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus the gradient for the biases is simply the back-propagated error signal &lt;script type=&quot;math/tex&quot;&gt;\delta_k&lt;/script&gt; from the output units. One interpretation of this is that the biases are weights on activations that are always equal to one, regardless of the feed-forward signal. Thus the bias gradients aren’t affected by the feed-forward signal, only by the error.&lt;/p&gt;

&lt;h1 id=&quot;gradients-for-hidden-layer-parameters&quot;&gt;Gradients for Hidden Layer Parameters&lt;/h1&gt;

&lt;p&gt;Now that we’ve derived the gradients for the output layer parameters and established the notion of backpropagation, let’s continue with this information in hand in order to derive the gradients for the remaining layers.&lt;/p&gt;

&lt;h4 id=&quot;hidden-layer-connection-weights-w_ij&quot;&gt;Hidden layer connection weights, &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Due to the indirect affect of the hidden layer on the output error, calculating the gradients for the hidden layer weights &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; is somewhat more involved. However, the process starts just the same as for the output layer &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial w_{ij}} &amp;= \frac{1}{2} \sum_{k}(a_k - t_k)^2 \\
&amp;= \sum_{k} (a_k - t_k) \frac{\partial}{\partial w_{ij}}a_k \tag{9}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Continuing on, noting that &lt;script type=&quot;math/tex&quot;&gt;a_k = g_k(z_k)&lt;/script&gt; and again applying chain rule, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial w_{ij}} &amp;= \sum_{k} (a_k - t_k) \frac{\partial }{\partial w_{ij}}g_k(z_k) \\
&amp;= \sum_{k} (a_k - t_k)g'_k(z_k)\frac{\partial }{\partial w_{ij}}z_k \tag{10}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Ok, now here’s where things get &lt;em&gt;slightly more involved&lt;/em&gt;. Notice that the partial derivative &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial }{\partial w_{ij}}z_k&lt;/script&gt; in &lt;em&gt;Equation 10&lt;/em&gt; is with respect to &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt;, but the target &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; is a function of index &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. How the heck do we deal with that!? If we expand &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; a little, we find that it is composed of other sub functions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} z_k &amp;= b_k + \sum_j a_jw_{jk} \\
&amp;= b_k + \sum_j g_j(z_j)w_{jk} \\
&amp;= b_k + \sum_j g_j(b_i + \sum_i a_i w_{ij})w_{jk} \tag{11}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;From &lt;em&gt;Equation 11&lt;/em&gt; we see that &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; is indirectly dependent on &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt;. &lt;em&gt;Equation 10&lt;/em&gt; also suggests that we can again use the chain rule to calculate &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_k }{\partial w_{ij}}&lt;/script&gt;. This is probably the trickiest part of the derivation, and also requires noting that &lt;script type=&quot;math/tex&quot;&gt;z_j = b_j + \sum_i a_jw_{ij}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a_j=g_j(z_j)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial z_k }{\partial w_{ij}} &amp;= \frac{\partial z_k}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}} \\
&amp;= \frac{\partial}{\partial a_j} (b_k + \sum_j a_jw_{jk}) \frac{\partial a_j}{\partial w_{ij}} \\
&amp;= w_{jk}\frac{\partial a_j}{\partial w_{ij}} \\
&amp;= w_{jk}\frac{\partial g_j(z_j)}{\partial w_{ij}} \\
&amp;= w_{jk}g_j'(z_j)\frac{\partial z_j}{\partial w_{ij}} \\
&amp;= w_{jk}g_j'(z_j)\frac{\partial}{\partial w_{ij}}(b_i + \sum_i a_i w_{ij}) \\
&amp;= w_{jk}g_j'(z_j)a_i \tag{12}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, plugging &lt;em&gt;Equation 12&lt;/em&gt; into &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_k}{\partial w_{ij}}&lt;/script&gt; into &lt;em&gt;Equation 10&lt;/em&gt; gives the following expression for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial E}{\partial w_{ij}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial w_{ij}} &amp;= \sum_{k} (a_k - t_k)g'_k(z_k)w_{jk} g'_j(z_j)a_i \\
&amp;= \left(\sum_{k} \color{purple}{\delta_k} w_{jk} \right) \color{darkblue}{g'_j(z_j)}\color{darkgreen}{a_i} \tag{13}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Notice that the gradient for the hidden layer weights has a similar form to that of the gradient for the output layer weights. Namely the gradient is composed of three terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the current layer’s activation function &lt;script type=&quot;math/tex&quot;&gt;\color{darkblue}{g'_j(z_j)}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;the output activation signal from the layer below &lt;script type=&quot;math/tex&quot;&gt;\color{darkgreen}{a_i}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;an error term  &lt;script type=&quot;math/tex&quot;&gt;\sum_{k} \color{purple}{\delta_k} w_{jk}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the output layer weight gradients, the error term was simply the difference in the target and output layer activations &lt;script type=&quot;math/tex&quot;&gt;\color{red}{a_k - t_k}&lt;/script&gt;. Here, the error term includes not only the output layer error signal, &lt;script type=&quot;math/tex&quot;&gt;\delta_k&lt;/script&gt;, but this error signal is further projected onto &lt;script type=&quot;math/tex&quot;&gt;w_{jk}&lt;/script&gt;. Analogous to the output layer weights, the gradient for the hidden layer weights can be interpreted as a proxy for the “contribution” of the weights to the output error signal. However, for hidden layers, this error can only be “observed” from the point-of-view of the weights by backpropagating the error signal through the layers above the hidden layer.&lt;/p&gt;

&lt;p&gt;To make this idea more explicit, we can define the resulting error signal backpropagated to layer &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\delta_j&lt;/script&gt;, which includes all terms in &lt;em&gt;Equation 13&lt;/em&gt; that involve index &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. This definition results in the following gradient for the hidden unit weights:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\color{darkred}{\delta_j} = \color{darkblue}{g'_j(z_j)} \sum_{k} \color{purple}{\delta_k} w_{jk} \tag{14}&lt;/script&gt;

&lt;p&gt;Thus giving the final expression for the gradient:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E }{\partial w_{ij}} = \color{darkred}{\delta_j}\color{darkgreen}{a_i}  \tag{15}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Equation 15&lt;/em&gt; suggests that &lt;strong&gt;&lt;em&gt;in order to calculate the weight gradients at any layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; in an arbitrarily-deep neural network, we simply need to calculate the backpropagated error signal &lt;script type=&quot;math/tex&quot;&gt;\delta_l&lt;/script&gt; that reaches that layer from the “above” layers, and weight it by the feed-forward signal &lt;script type=&quot;math/tex&quot;&gt;a_{l-1}&lt;/script&gt; feeding into that layer.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;hidden-layer-biases-b_i&quot;&gt;Hidden Layer Biases, &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Calculating the error gradients with respect to the hidden layer biases follows a very similar procedure to that for the hidden layer weights where, as in &lt;em&gt;Equation 12&lt;/em&gt;, we use the chain rule to calculate &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_k}{\partial b_i}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}

\frac{\partial E }{\partial b_{i}} &amp;= \sum_{k} (a_k - t_k) \frac{\partial }{\partial b_{i}}g_k(z_k) \\
&amp;= \sum_{k} (a_k - t_k)g'_k(z_k)\frac{\partial z_k}{\partial b_{i}}  \tag{16}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Again, using the chain rule to solve for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_k }{\partial b_{i}}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial z_k  }{\partial b_{i}} &amp;= \frac{\partial z_k}{\partial a_j}\frac{\partial a_j}{\partial b_{i}} \\
&amp;= \frac{\partial}{\partial a_j}(b_j + \sum_j a_j w_{jk})\frac{\partial a_j}{\partial b_{i}} \\
&amp;= w_{jk}\frac{\partial a_j}{\partial b_{i}} \\
&amp;= w_{jk}\frac{\partial g_j(z_j)}{\partial b_{i}} \\
&amp;= w_{jk}g_j'(z_j)\frac{\partial z_j}{\partial b_{i}} \\
&amp;= w_{jk}g_j'(z_j)\frac{\partial}{\partial b_i}(b_i + \sum_i a_i w_{ij}) \\
&amp;= w_{jk}g_j'(z_j)(1) \tag{17}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Plugging &lt;em&gt;Equation 17&lt;/em&gt; into the expression for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_k }{\partial b_i}&lt;/script&gt; in &lt;em&gt;Equation 16&lt;/em&gt; gives the final expression for the hidden layer bias gradients:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E }{\partial b_i} &amp;= \sum_{k} (a_k - t_k)g'_k(z_k) w_{jk} g_j'(z_j) \\
&amp;= g'_j(z_j) \sum_{k} \delta_k w_{jk} \\
&amp;= \color{darkred}{\delta_j} \tag{18}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In a similar fashion to calculation of the bias gradients for the output layer, the gradients for the hidden layer biases are simply the backpropagated error signal reaching that layer. This suggests that we can also calculate the bias gradients at any layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; in an arbitrarily-deep network by simply calculating the backpropagated error signal reaching that layer &lt;script type=&quot;math/tex&quot;&gt;\delta_l&lt;/script&gt;. Pretty cool!&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h1&gt;

&lt;p&gt;In this post we went over some of the formal details of the backpropagation learning algorithm. The math covered in this post allows us to train arbitrarily deep neural networks by re-applying the same basic computations. In a later post, we’ll go a bit deeper in implementation and applications of neural networks, referencing this post for the formal development of the underlying calculus required for gradient descent.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;notes&quot;&gt;Notes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Though, I guess these days with autograd, who &lt;em&gt;really&lt;/em&gt; needs to understand how the calculus for gradient descent works, amiright? (&lt;em&gt;hint&lt;/em&gt;: that is a joke) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;You may also notice that the summation disappears in the derivative. This is because when we take the partial derivative with respect to the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;-th dimension/node. Therefore the only term that survives in the error gradient is the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;-th, and we can thus ignore the remaining terms in the summation. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Notice here that the sum does &lt;em&gt;not&lt;/em&gt; disappear in the derivative as it did for the output layer parameters. This is due to the fact that the hidden layers are fully connected, and thus each of the hidden unit outputs affects the state of each output unit. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Dustin Stansbury</name></author><category term="neural-networks" /><category term="gradient-descent" /><category term="derivation" /><summary type="html">Artificial neural networks (ANNs) are a powerful class of models used for nonlinear regression and classification tasks that are motivated by biological neural computation. The general idea behind ANNs is pretty straightforward: map some input onto a desired target value using a distributed cascade of nonlinear transformations (see Figure 1). However, for many, myself included, the learning algorithm used to train ANNs can be difficult to get your head around at first. In this post I give a step-by-step walkthrough of the derivation of the gradient descent algorithm commonly used to train ANNs–aka the “backpropagation” algorithm. Along the way, I’ll also try to provide some high-level insights into the computations being performed during learning1. Though, I guess these days with autograd, who really needs to understand how the calculus for gradient descent works, amiright? (hint: that is a joke) &amp;#8617;</summary></entry><entry><title type="html">Derivation: Derivatives for Common Neural Network Activation Functions</title><link href="https://dustinstansbury.github.io/theclevermachine/derivation-common-neural-network-activation-functions" rel="alternate" type="text/html" title="Derivation: Derivatives for Common Neural Network Activation Functions" /><published>2020-06-29T00:00:00-07:00</published><updated>2020-06-29T00:00:00-07:00</updated><id>https://dustinstansbury.github.io/theclevermachine/derivation-common-neural-network-activation-functions</id><content type="html" xml:base="https://dustinstansbury.github.io/theclevermachine/derivation-common-neural-network-activation-functions">&lt;p&gt;When constructing Artificial Neural Network (ANN) models, one of the primary considerations is choosing activation functions for hidden and output layers that are differentiable. This is because calculating the backpropagated error signal that is used to determine ANN parameter updates requires the gradient of the activation function gradient . Three of the most commonly-used activation functions used in ANNs are the identity function, the logistic sigmoid function, and the hyperbolic tangent function. Examples of these functions and their associated gradients (derivatives in 1D) are plotted in Figure 1.&lt;/p&gt;

&lt;hr /&gt;
&lt;center&gt;
    &lt;br /&gt;
    &lt;div id=&quot;container&quot;&gt;
        &lt;img width=&quot;800&quot; src=&quot;assets/images/a-gentle-introduction-to-neural-networks/common_activation_functions.png&quot; /&gt;
    &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Figure 1:&lt;/em&gt;&lt;/strong&gt; Common activation functions functions used in artificial neural, along with their derivatives&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In the remainder of this post, we derive the derivatives/gradients for each of these common activation functions.&lt;/p&gt;

&lt;h1 id=&quot;the-identity-activation-function&quot;&gt;The Identity Activation Function&lt;/h1&gt;

&lt;p&gt;The simplest activation function, one that is commonly used for the output layer activation function in regression problems,  is the identity/linear activation function (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, red curves):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_{linear}(z) = z&lt;/script&gt;

&lt;p&gt;This activation function simply maps the pre-activation to itself and can output values that range &lt;script type=&quot;math/tex&quot;&gt;(-\infty, \infty)&lt;/script&gt;. Why would one want to do use an identity activation function? After all, a multi-layered network with linear activations at each layer can be equally-formulated as a single-layered linear network. It turns out that the identity activation function is surprisingly useful. For example, a multi-layer network that has nonlinear activation functions amongst the hidden units and an output layer that uses the identity activation function implements a powerful form of nonlinear regression. Specifically, the network can predict continuous target values using a linear combination of signals that arise from one or more layers of nonlinear transformations of the input.&lt;/p&gt;

&lt;p&gt;The derivative of &lt;script type=&quot;math/tex&quot;&gt;g_{\text{linear}}&lt;/script&gt; ,  &lt;script type=&quot;math/tex&quot;&gt;g'_{\text{linear}}&lt;/script&gt;,  is simply 1, in the case of 1D inputs. For vector inputs of length D the gradient is &lt;script type=&quot;math/tex&quot;&gt;\vec{1}^{1 \times D}&lt;/script&gt;, a vector of ones of length D.&lt;/p&gt;

&lt;h1 id=&quot;the-logistic-sigmoid-activation-function&quot;&gt;The Logistic Sigmoid Activation Function&lt;/h1&gt;

&lt;p&gt;Another function that is often used as the output activation function for binary classification problems (i.e. outputs values that range (0, 1)), is the logistic sigmoid (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, blue curves). The logistic sigmoid has the following form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{rcl} g_{\text{logistic}}(z) = \frac{1}{1 + e^{-z}}\end{array}&lt;/script&gt;

&lt;p&gt;and outputs values that range (0, 1). The logistic sigmoid is motivated somewhat by biological neurons and can be interpreted as the probability of an artificial neuron “firing” given its inputs. (It turns out that the logistic sigmoid can also be derived as the maximum likelihood solution to for logistic regression in statistics). Calculating the derivative of the logistic sigmoid function makes use of the quotient rule and a clever trick that both adds and subtracts a one from the numerator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g'_{\text{logistic}}(z) &amp;=&amp; \frac{\partial}{\partial z} \left ( \frac{1}{1 + e^{-z}}\right ) \\  &amp;=&amp; \frac{e^{-z}}{(1 + e^{-z})^2} \text{(by chain rule)} \\  &amp;=&amp; \frac{1 + e^{-z} - 1}{(1 + e^{-z})^2} \\  &amp;=&amp; \frac{1 + e^{-z}}{(1 + e^{-z})^2} - \left ( \frac{1}{1+e^{-z}} \right )^2 \\  &amp;=&amp; \frac{1}{(1 + e^{-z})} - \left ( \frac{1}{1+e^{-z}} \right )^2 \\  &amp;=&amp; g_{\text{logistic}}(z)- g_{\text{logistic}}(z)^2 \\  &amp;=&amp; g_{\text{logistic}}(z)(1 - g_{\text{logistic}}(z)) \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here we see that &lt;script type=&quot;math/tex&quot;&gt;g'_{logistic}(z)&lt;/script&gt; evaluated at &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is simply &lt;script type=&quot;math/tex&quot;&gt;g_{logistic}(z)&lt;/script&gt; weighted by &lt;script type=&quot;math/tex&quot;&gt;(1-g_{logistic}(z))&lt;/script&gt;. This turns out to be a convenient form for efficiently calculating gradients used in neural networks: if one keeps in memory the feed-forward activations of the logistic function for a given layer, the gradients for that layer can be evaluated using simple multiplication and subtraction rather than performing any re-evaluating the sigmoid function, which requires extra exponentiation.&lt;/p&gt;

&lt;h1 id=&quot;the-hyperbolic-tangent-activation-function&quot;&gt;The Hyperbolic Tangent Activation Function&lt;/h1&gt;

&lt;p&gt;Though the logistic sigmoid has a nice biological interpretation, it turns out that the logistic sigmoid can cause a neural network to get “stuck” during training. This is due in part to the fact that if a strongly-negative input is provided to the logistic sigmoid, it outputs values very near zero. Since neural networks use the feed-forward activations to calculate parameter gradients (again, see this &lt;a href=&quot;https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/&quot;&gt;this post&lt;/a&gt; for details), this can result in model parameters that are updated less regularly than we would like, and are thus “stuck” in their current state.&lt;/p&gt;

&lt;p&gt;An alternative to the logistic sigmoid is the hyperbolic tangent, or &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}&lt;/script&gt; function (&lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;, green curves):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g_{\text{tanh}}(z) &amp;=&amp; \frac{\text{sinh}(z)}{\text{cosh}(z)} \\  &amp;=&amp; \frac{\mathrm{e}^z - \mathrm{e}^{-z}}{\mathrm{e}^z + \mathrm{e}^{-z}}\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Like the logistic sigmoid, the tanh function is also sigmoidal (“s”-shaped), but instead outputs values that range &lt;script type=&quot;math/tex&quot;&gt;(-1, 1)&lt;/script&gt;. Thus strongly negative inputs to the tanh will map to negative outputs. Additionally, only zero-valued inputs are mapped to near-zero outputs. These properties make the network less likely to get “stuck” during training. Calculating the gradient for the tanh function also uses the quotient rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} g'_{\text{tanh}}(z) &amp;=&amp; \frac{\partial}{\partial z} \frac{\text{sinh}(z)}{\text{cosh}(z)} \\  &amp;=&amp; \frac{\frac{\partial}{\partial z} \text{sinh}(z) \times \text{cosh}(z) - \frac{\partial}{\partial z} \text{cosh}(z) \times \text{sinh}(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; \frac{\text{cosh}^2(z) - \text{sinh}^2(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; 1 - \frac{\text{sinh}^2(z)}{\text{cosh}^2(z)} \\  &amp;=&amp; 1 - \text{tanh}^2(z)\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similar to the derivative for the logistic sigmoid, the derivative of &lt;script type=&quot;math/tex&quot;&gt;g_{\text{tanh}}(z)&lt;/script&gt; is a function of feed-forward activation evaluated at z, namely &lt;script type=&quot;math/tex&quot;&gt;(1-g_{\text{tanh}}(z)^2)&lt;/script&gt;. Thus the same caching trick can be used for layers that implement &lt;script type=&quot;math/tex&quot;&gt;\text{tanh}&lt;/script&gt; activation functions.&lt;/p&gt;

&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;In this post we reviewed a few commonly-used activation functions in neural network literature and their derivative calculations. These activation functions are motivated by biology and/or provide some handy implementation tricks like calculating derivatives using cached feed-forward activation values. Note that there are also many other options for activation functions not covered here: e.g. rectification, soft rectification, polynomial kernels, etc. Indeed, finding and evaluating novel activation functions is an active subfield of machine learning research. However, the three basic activations covered here can be used to solve a majority of the machine learning problems one will likely face.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;</content><author><name>Dustin Stansbury</name></author><category term="neural-networks" /><category term="gradient-descent" /><category term="derivation" /><summary type="html">When constructing Artificial Neural Network (ANN) models, one of the primary considerations is choosing activation functions for hidden and output layers that are differentiable. This is because calculating the backpropagated error signal that is used to determine ANN parameter updates requires the gradient of the activation function gradient . Three of the most commonly-used activation functions used in ANNs are the identity function, the logistic sigmoid function, and the hyperbolic tangent function. Examples of these functions and their associated gradients (derivatives in 1D) are plotted in Figure 1.</summary></entry><entry><title type="html">Derivation: Ordinary Least Squares Solution and Normal Equations</title><link href="https://dustinstansbury.github.io/theclevermachine/derivation-ols-normal-equations" rel="alternate" type="text/html" title="Derivation: Ordinary Least Squares Solution and Normal Equations" /><published>2020-06-29T00:00:00-07:00</published><updated>2020-06-29T00:00:00-07:00</updated><id>https://dustinstansbury.github.io/theclevermachine/derivation-ols-normal-equations</id><content type="html" xml:base="https://dustinstansbury.github.io/theclevermachine/derivation-ols-normal-equations">&lt;p&gt;In a linear regression framework, we assume some output variable &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is a linear combination of some independent input variables &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; plus some independent noise &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;. The way the independent variables are combined is defined by a parameter vector &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} y &amp;=&amp; X \beta + \epsilon \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;We also assume that the noise term &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is drawn from a standard Normal distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}\epsilon &amp;\sim&amp; N(0,I)\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;For some estimate of the model parameters &lt;script type=&quot;math/tex&quot;&gt;\hat \beta&lt;/script&gt;, the model’s prediction errors &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; are the difference between the model prediction and the observed ouput values&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{rcl} e = y - X\hat \beta \end{array}&lt;/script&gt;

&lt;p&gt;The Ordinary Least Squares (OLS) solution to the problem (i.e. determining an optimal solution for &lt;script type=&quot;math/tex&quot;&gt;\hat \beta&lt;/script&gt;) involves minimizing the sum of the squared errors with respect to the model parameters, &lt;script type=&quot;math/tex&quot;&gt;\hat \beta&lt;/script&gt;. The sum of squared errors is equal to the inner product of the residuals vector with itself &lt;script type=&quot;math/tex&quot;&gt;\sum e_i^2 = e^Te&lt;/script&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} e^T e &amp;=&amp; (y - X \hat \beta)^T (y - X \hat \beta) \\  &amp;=&amp; y^Ty - y^T (X \hat \beta) - (X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\  &amp;=&amp; y^Ty - (X \hat \beta)^T y - (X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\  &amp;=&amp; y^Ty - 2(X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\  &amp;=&amp; y^Ty - 2\hat \beta^T X^T y + \hat \beta^T X^T X \hat \beta \\  \end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;To determine the parameters, &lt;script type=&quot;math/tex&quot;&gt;\hat \beta&lt;/script&gt;, we minimize the sum of squared residuals with respect to the parameters.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
\frac{\partial}{\partial \beta} \left[ e^T e \right] &amp;=&amp; 0 \\  
-2X^Ty + 2X^TX \hat \beta &amp;=&amp; 0 \text{, and thus} \\ 
X^TX \hat \beta   &amp;=&amp; X^Ty
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;due to the identity &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \mathbf{a}^T \mathbf{b}}{\partial \mathbf{a}} = \mathbf{b}&lt;/script&gt;, for vectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{a}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b}&lt;/script&gt;. This relationship is matrix form of the Normal Equations. Solving for &lt;script type=&quot;math/tex&quot;&gt;\hat \beta&lt;/script&gt; gives  the analytical solution to the Ordinary Least Squares problem.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl} \hat \beta &amp;=&amp; (X^TX)^{-1}X^Ty \end{array} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;
&lt;hr /&gt;</content><author><name>Dustin Stansbury</name></author><category term="derivation" /><category term="ordinary-least-squares" /><summary type="html">In a linear regression framework, we assume some output variable is a linear combination of some independent input variables plus some independent noise . The way the independent variables are combined is defined by a parameter vector :</summary></entry></feed>