<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="stylesheet" href="/theclevermachine/assets/main.css"">
  <link rel=" icon" type="image/png" href="icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>SVD and Data Compression Using Low-rank Matrix Approximation | The Clever Machine</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="SVD and Data Compression Using Low-rank Matrix Approximation" />
<meta name="author" content="Dustin Stansbury" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In a previous post we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA." />
<meta property="og:description" content="In a previous post we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA." />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/svd-data-compression" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/svd-data-compression" />
<meta property="og:site_name" content="The Clever Machine" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-16T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="SVD and Data Compression Using Low-rank Matrix Approximation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Dustin Stansbury"},"dateModified":"2020-08-16T00:00:00-07:00","datePublished":"2020-08-16T00:00:00-07:00","description":"In a previous post we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA.","headline":"SVD and Data Compression Using Low-rank Matrix Approximation","mainEntityOfPage":{"@type":"WebPage","@id":"https://dustinstansbury.github.io/theclevermachine/svd-data-compression"},"url":"https://dustinstansbury.github.io/theclevermachine/svd-data-compression"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a><a class="page-link" href="/theclevermachine/topics/">Topics</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">SVD and Data Compression Using Low-rank Matrix Approximation</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-08-16T00:00:00-07:00" itemprop="datePublished"><i class="fa-solid fa-calendar"></i> Aug 16, 2020</time>
      <br><span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card"
          itemprop="name"> <i class="fa-solid fa-pencil"> </i> Dustin Stansbury
        </span>
      </span><br>
      <i class="fa-solid fa-tags"></i><span itemprop="tags">
        
        
        
        <a href="/theclevermachine/topics/linear-algebra.html">linear-algebra</a>
        , 
        
        <a href="/theclevermachine/topics/singular-value-decomposition.html">singular-value-decomposition</a>
        , 
        
        <a href="/theclevermachine/topics/low-rank-approximation.html">low-rank-approximation</a>
        , 
        
        <a href="/theclevermachine/topics/data-compression.html">data-compression</a>
        , 
        
        <a href="/theclevermachine/topics/image-compression.html">image-compression</a>
        
        
        

      </span></p>

  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In a <a href="/theclevermachine/singular-value-decomposition">previous post</a> we introduced the Singular Value Decomposition (SVD) and its many advantages and applications. In this post, we’ll discuss one of my favorite applications of SVD: data compression using low-rank matrix approximation (LRA). We’ll start off with a quick introduction to LRA and how it relates to data compression. Then we’ll demonstrate how SVD provides a convenient and intuitive method for image compression using a LRA.</p>

<h1 id="data-compression-and-low-rank-approximation">Data Compression and Low-Rank Approximation</h1>

<p>First off, what do we mean by low-rank approximation? Say you have an \(m \times n\) data matrix \(X\). The data contained in \(X\) could be anything, really. For example, in a computer vision setting, \(X\) could encode a single image, where each entry in the matrix is a pixel intensity value at a location encoded by the \(i,j\)-th row and column. In a machine learning setting, \(X\) could be a data set, where each row is an observation and each column is a measurable dimension. Heck, in a computer-vision-meets-machine-learning setting, \(X\) could represent multiple images, with each image being encoded as a row, and each column being one of \(n = (\text{width} \times \text{height}\)) values, encoding the image location-pixel values unraveled into a row-vector.</p>

<p>No matter the type of information \(X\) encodes, it will have a <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)"><em>matrix rank</em></a> \(r\), which is essentially the number of linearly independent columns (column rank) or rows (row rank) contained in the matrix. We’ll focus on column rank in this post. It’s entirely possible (and common) for a matrix to have a rank that is smaller than the number of columns in the matrix. For example, the left two plots in <strong><em>Figure 1</em></strong> display two different matrices \(X\) and \(\tilde X\). These two matrices have the same column rank, despite having a different numbers of columns. This is because the matrix \(X\) is full rank in that its column rank is equal to the number of columns. In contrast, the matrix \(\tilde X\) contains redundant columns, resulting in a column rank that is smaller than the number of columns.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p><br /></p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="650" src="assets/images/svd-data-compression/matrix-low-rank-approximation.png" />
    </div>
</center>

<p><strong><em>Figure 1: Matrix Rank and Reconstruction.</em></strong>
<em><strong>Left:</strong> a full-column-rank matrix \(X\). <strong>Middle:</strong> a matrix \(\tilde X\) with redundant columns formed by scaling and concatenating columns of \(X\). <strong>Right</strong>: exact reconstruction of \(\tilde X\) using a rank \(k=r=4\) singular value decomposition.</em></p>

<details>
  <summary>Python Code</summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'image.cmap'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'RdBu_r'</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span> <span class="c1"># Repeatability
</span><span class="n">MATRIX_RANK</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Create a random matrix, which will have independent columns
# and thus a rank equal to the number of columns
</span><span class="n">X_orig</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">MATRIX_RANK</span><span class="p">)</span>
<span class="n">X_orig_rank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_orig</span><span class="p">)</span>

<span class="c1"># Create a new redundant matrix with twice as many columns, but new columns are
# simply a linear scaling of original matrix
</span><span class="n">X_redundant</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">X_orig</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X_orig</span><span class="p">])</span>

<span class="c1"># Permute columns of redundant matrix
</span><span class="n">X_redundant</span> <span class="o">=</span> <span class="n">X_redundant</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_redundant</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))]</span>
<span class="n">X_redundant_rank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_redundant</span><span class="p">)</span>

<span class="c1"># Run SVD on redundant matrix, we'll use this for LRA
</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X_redundant</span><span class="p">)</span>

<span class="c1"># Low-rank reconstruction (exact in this case)
</span><span class="n">X_redundant_reconstructed</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">MATRIX_RANK</span><span class="p">]</span> <span class="o">*</span> <span class="n">S</span><span class="p">[:</span><span class="n">MATRIX_RANK</span><span class="p">]</span> <span class="o">@</span> <span class="n">V</span><span class="p">[:</span><span class="n">MATRIX_RANK</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">reconstruction_error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">X_redundant</span> <span class="o">-</span> <span class="n">X_redundant_reconstructed</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Visualizations
## Original matrix
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_orig</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"A Rank $r=$</span><span class="si">{</span><span class="n">X_orig_rank</span><span class="si">}</span><span class="s"> matrix, X"</span><span class="p">)</span>

<span class="c1">## Redundant matrix
</span><span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_redundant</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"A redundant Rank $r=$</span><span class="si">{</span><span class="n">X_redundant_rank</span><span class="si">}</span><span class="s"> matrix, $</span><span class="se">\\</span><span class="s">tilde X$"</span><span class="p">)</span>

<span class="c1">## Low-rank approximation (exact reconstruction)
</span><span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_redundant_reconstructed</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Reconstruction of $</span><span class="se">\\</span><span class="s">tilde X$</span><span class="se">\n</span><span class="s">using </span><span class="si">{</span><span class="n">MATRIX_RANK</span><span class="si">}</span><span class="s"> components of SVD</span><span class="se">\n</span><span class="s">Total Squared Error: </span><span class="si">{</span><span class="n">reconstruction_error</span><span class="p">:</span><span class="mf">1.1</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Low-Rank Approximation of a Matrix"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
</code></pre></div>  </div>
</details>

<hr />

<p><br /></p>

<p>When a matrix like \(\tilde X\) contains redundant information, that matrix can often be <em>compressed</em>: i.e. it can be represented using less data than the original matrix with little-to-no loss in information. One way to perform compression is by using LRA.</p>

<p>Low-rank approximation (<strong><em>Figure 2</em></strong>) is the process of representing the information in a matrix \(M\) using a matrix \(\hat M\) that has a rank that is smaller than the original matrix. To reduce the rank of \(\hat M\) we can attempt construct the matrix as a combination of a “tall” left-hand matrix \(L_k\) and a “wide” right-hand matrix \(R_k^T\):</p>

\[\begin{align}
M &amp;= L R^T \\
&amp;\approx L_k R_k^T \\
&amp;\approx \hat M \tag{1}

\end{align}\]

<p>This allows a matrix that would normally be represented using \(m \times n\) values to be represented using \(k(m + n)\) values. If \(k\) is small relative to \(m\) and \(n\), then the LRA can be used to store important information in \(M\) much more efficiently.</p>

<p><br /></p>

<hr />

<center>
    <br />
    <div id="container">
        <img width="500" src="assets/images/svd-data-compression/low-rank-approximation.png" />
    </div>
</center>

<p><strong><em>Figure 2: Low-rank Matrix Decomposition:</em></strong>
<em>A matrix \(M\) of size \(m \times n\) and rank \(r\) can be decomposed into a pair of matrices \(L_k\) and \(R_k\). When \(k=r\), the matrix \(M\) can be exactly reconstructed from the decomposition. When \(k &lt; r\), then the decomposition provides a low-rank approximation \(\hat M\) of \(M\).</em></p>

<hr />

<p><br /></p>

<p>Low-rank approximation is often useful when the matrix \(M\) contains information that can be ignored, such as redundant information, or irrelevant information that isn’t helpful, or can possibly even be detrimental for solving a particular numerical problem (e.g. noise).</p>

<p>There are a number of methods for constructing the matrix \(\hat M\), but a common method is to use <a href="/theclevermachine/singular-value-decomposition">Singular Value Decomposition (SVD)</a>. Specifically, SVD decomposes matrix \(M\) into three matrices:</p>

\[\begin{align}
M &amp;= USV^T \\
&amp;= (US) V^T \\
&amp;= L R^T \text{, where} \\
L &amp;= (US) \text{, and} \\
R &amp;= V \tag{2}
\end{align}\]

<p>When full-rank SVD is used, <strong><em>Equation 2</em></strong> provides a method to <em>exactly</em> reconstruct \(M\). In a similar fashion, <strong><em>Figure 1, right</em></strong> demonstrates how SVD can be used used to <em>exactly</em> reconstruct the redundant matrix \(\tilde X\) using a decomposition of rank \(k=r=4\), despite the matrix \(\tilde X\) having 8 columns.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>However, we’re not limited to exact reconstruction of \(M\); SVD offers a straight-forward way to obtain a low-rank approximation of \(M\). We can replace \(U\), \(S\), and \(V\) in <strong><em>Equation 2</em></strong> with \(U_k\), \(S_k\), and \(V_k\), where we use only the first \(k\) columns of the decomposition matrices:</p>

\[\begin{align}
M &amp;\approx U_kS_kV_k^T \\
&amp;\approx \hat M_k. \tag{3}
\end{align}\]

<p>When \(k &lt; r\) then <strong><em>Equation 3</em></strong> provides a LRA of \(M\), \(\hat M_k\), via SVD. We’ll demonstrate this more clearly with an example that uses SVD for image compression.</p>

<p><br /></p>

<h1 id="application-image-compression">Application: Image Compression</h1>

<p>Singular value decomposition can be used to decompose <em>any</em> matrix, which allows us to use SVD to compress all sorts of data, including images. <strong><em>Figure 3, left</em></strong> depicts a grayscale image, encoded as a data matrix \(X\) with rank \(r=128\). When SVD is applied to \(X\), it returns a set of left singular vectors \(U,\) right singular vectors \(V\), and a diagonal matrix \(S\) that contains the singular values associated with the singular vectors.</p>

<p>SVD is great because the singular vectors and values are rank-ordered in such a way that earlier components carry the most information about \(X\). The singular values in \(S\) (<strong><em>Figure 3, center</em></strong>) can be used as a proxy for the amount of information in \(X\) encoded in each component of the decomposition (<strong><em>Figure 3, right</em></strong>).</p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="800" src="assets/images/svd-data-compression/image-singular-values.png" />
    </div>
</center>

<p><strong><em>Figure 3: Singular Value Decomposition of an image \(X\).</em></strong> <em><strong>Left:</strong> A Grayscale image can be interpreted as a matrix \(X\). <strong>Center:</strong> the singular values (blue) and their log (red) as a function of rank \(k.\) Singular values decrease exponentially with rank, with earlier singular values being much larger than later ones. <strong>Right:</strong> The total information about \(X\) encoded in all the singular values up to \(k.\) A majority of information is encoded in the first singular vectors returned by SVD.</em></p>

<details>
  <summary>Python Code</summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load image
</span><span class="n">img</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"../assets/images/svd-data-compression/cameraman.png"</span><span class="p">)</span>

<span class="c1"># Donwsample and encode RGBa image as matrix of intensities, X
</span><span class="n">DOWNSAMPLE</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">img</span><span class="p">[::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="p">::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">img</span><span class="p">[::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="p">::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">img</span><span class="p">[::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="p">::</span><span class="n">DOWNSAMPLE</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> 
<span class="n">X</span> <span class="o">=</span> <span class="mf">0.2989</span> <span class="o">*</span> <span class="n">R</span> <span class="o">+</span> <span class="mf">0.5870</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="mf">0.1140</span> <span class="o">*</span> <span class="n">B</span>

<span class="c1"># Calculate the rank of the data matrix, X
</span><span class="n">img_rank</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>

<span class="c1"># Run SVD on Image
</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Calculate the cumulative variance explained by each singular value
</span><span class="n">total_S</span> <span class="o">=</span> <span class="n">S</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">component_idx</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>  <span class="n">n_components</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">info_retained</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_S</span>

<span class="c1"># Visualizations
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1">## Raw Image, X
</span><span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Matrix $X$ encoding a Grayscale Image</span><span class="se">\n</span><span class="s">(Rank, $r=$</span><span class="si">{</span><span class="n">img_rank</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>

<span class="c1">## Singular values as function of rank
</span><span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1">### Raw singular values
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">component_idx</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Singular Values of $$X$$'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkblue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Rank, $k$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$S_k$'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkblue'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s">'darkblue'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Singular Values, $S_k$'</span><span class="p">)</span>

<span class="c1">### log(singular values)
</span><span class="n">twax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">().</span><span class="n">twinx</span><span class="p">()</span>  <span class="c1"># twin axes that shares the same x-axis
</span><span class="n">twax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">component_idx</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">S</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'$\log(S_k)$</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">labelcolor</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

<span class="c1">## Information retained as function of rank
</span><span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">component_idx</span><span class="p">,</span> <span class="n">info_retained</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkgreen'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">105</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Rank, $k$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Cumulative % of Information About $X$</span><span class="se">\n</span><span class="s">Carried by Singular Vectors'</span><span class="p">)</span>
</code></pre></div>  </div>
</details>

<hr />
<p><br /></p>

<p>We can see in <strong><em>Figure 3, center, right</em></strong> that a majority of the information about \(X\) is encoded in the first handfull of singular vectors/values returned by SVD. For example, 80% of information is endoded by less than a \(1/3\) of the singular vectors. This suggest that we can encode a majority of the information about the original data using only a subset of SVD components, and that it is easy to identify the optimal subset.</p>

<p><strong><em>Figure 4</em></strong> demonstrates this idea. In each row of <strong><em>Figure 4</em></strong> we reconstruct \(X\) while increasing the rank \(k\) used in the reconstruction.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> Using only a few singular vectors (e.g. \(k=4\)) limits the reconstruction \(\hat X_k\) to encode only low-frequency spatial information about the image. As the number of singular vectors used in the approximation increases, the reconstruction includes increasing high-frequency spatial information, and thus decreasing the reconstruction error.</p>

<p>Using roughly 50% of the data required to store \(X\) (\(k=32)\) provides around 80% of the information in \(X,\) and the reconstruction is almost perceptually indistinguishable from the original image. We can also see that this approach isn’t a magic bullet. There’s a trade-off between the amount of data required for the reconstruction (i.e requirements for the components of \(U_k\), \(V_k\), and \(S_k\)) and the information provided about \(X\). Using 64 components results in basically no overall compression, but less than 100% information encoded. Effects like these need to be considered when using LRA for image compression.</p>

<p><br /></p>

<hr />
<center>
    <br />
    <div id="container">
        <img width="650" src="assets/images/svd-data-compression/svd-image-reconstruction.png" />
    </div>
</center>

<p><strong><em>Figure 4: Image Compression via LRA/SVD.</em></strong> <em><strong>Top Left</strong> Matrix \(X\) encodes an image that we reconstruct using an increasing number of left singular vectors provided by SVD. <strong>Second Column:</strong> The approximation \(\hat{X}_k\) of image \(X\) using the first \(k\) most-informative left singular vectors. <strong>Third column:</strong> The spatial reconstruction error using approximation \(\hat{X}_k\). <strong>Right Column:</strong> displays data compression information for each row. Information includes the percentage of original image size used to represent the approximation, as well as the amount of information about \(X\) contained in the approximation.</em></p>

<details>
  <summary>Python Code</summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Image Reconstruction
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"$X$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

<span class="c1"># Reconstruct image with increasing number of singular vectors/values
</span><span class="k">for</span> <span class="n">power</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">power</span><span class="p">)</span>

    <span class="c1"># Compressed/Reconstructed Image
</span>    <span class="n">X_reconstruction</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank</span><span class="p">]</span> <span class="o">*</span> <span class="n">S</span><span class="p">[:</span><span class="n">rank</span><span class="p">]</span> <span class="o">@</span> <span class="n">V</span><span class="p">[:</span><span class="n">rank</span><span class="p">,:]</span>

    <span class="c1"># Calculate number of floats required to store compressed image
</span>    <span class="n">rank_data_compression</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank</span><span class="p">].</span><span class="n">size</span> <span class="o">+</span> <span class="n">S</span><span class="p">[:</span><span class="n">rank</span><span class="p">].</span><span class="n">size</span> <span class="o">+</span> <span class="n">V</span><span class="p">[:</span><span class="n">rank</span><span class="p">,:].</span><span class="n">size</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>

    <span class="c1"># Variance of original image explained by n components
</span>    <span class="n">rank_info_retained</span> <span class="o">=</span> <span class="n">info_retained</span><span class="p">[</span><span class="n">rank</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Visualizations
</span>    <span class="c1">## Original Image
</span>    <span class="k">if</span> <span class="n">power</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="n">power</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">cla</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

    <span class="c1">## Image reconstruction
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="n">power</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_reconstruction</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'$\hat_$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">## Reconstruction error
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="n">power</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_reconstruction</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">clim</span><span class="p">([</span><span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'$X -\hat_$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">## Compression/reconstruction info
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">sca</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="n">power</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">compression_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'Compression: </span><span class="si">{</span><span class="n">rank_data_compression</span><span class="p">:</span><span class="mf">1.1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n</span><span class="s">Info. Retained </span><span class="si">{</span><span class="n">rank_info_retained</span><span class="p">:</span><span class="mf">1.1</span><span class="n">f</span><span class="si">}</span><span class="s">%'</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">4</span><span class="p">,</span> <span class="n">compression_text</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    
<span class="n">fig</span><span class="p">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">pad</span><span class="o">=</span><span class="p">.</span><span class="mi">01</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">'horizontal'</span><span class="p">)</span>
</code></pre></div>  </div>

</details>

<hr />

<p><br /></p>

<h1 id="wrapping-up">Wrapping Up</h1>

<p>In this post we discussed one of many applications of SVD: compression of high-dimensional data via LRA. This application is closely related to other numerical techniques such as denoising and matrix completion, as well as statistical analysis techniques for dimensionality reduction like Principal Components Analysis (PCA). Stay tuned, as I plan to dig into these additional applications of SVD in future posts. Until then, happy compressing!</p>

<h1 id="notes">Notes</h1>
<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It turns out these redundant columns have been generated by scaling and concatenating multiple columns from the full-rank matrix \(X\). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>This isn’t low-rank approximation, per se, since \(k=r\). However, it <em>does</em> demonstrate an important concept: redundancy can be compressed using a subset of components returned from matrix decomposition. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>In a normal compression scenario, rather than calculating the full SVD and selecting a subset of components, we would simply calculate a low-rank SVD, which can be done more efficiently than the full SVD. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'the-clever-machine'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/theclevermachine/svd-data-compression" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
