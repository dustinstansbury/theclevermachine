<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <link rel="stylesheet" href="/theclevermachine/assets/main.css">
  <link rel="icon"  type="image/png"    href="icon.png"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Derivation: Ordinary Least Squares Solution and the Normal Equations | The Clever Machine</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Derivation: Ordinary Least Squares Solution and the Normal Equations" />
<meta name="author" content="Dustin Stansbury" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Have you ever performed linear regression involving multiple predictor variables and run into this expression (\hat \beta = (X^TX)^{-1}X^Ty)? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!" />
<meta property="og:description" content="Have you ever performed linear regression involving multiple predictor variables and run into this expression (\hat \beta = (X^TX)^{-1}X^Ty)? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!" />
<link rel="canonical" href="https://dustinstansbury.github.io/theclevermachine/derivation-normal-equations" />
<meta property="og:url" content="https://dustinstansbury.github.io/theclevermachine/derivation-normal-equations" />
<meta property="og:site_name" content="The Clever Machine" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-23T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Derivation: Ordinary Least Squares Solution and the Normal Equations" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Dustin Stansbury"},"dateModified":"2020-07-23T00:00:00-07:00","datePublished":"2020-07-23T00:00:00-07:00","description":"Have you ever performed linear regression involving multiple predictor variables and run into this expression (\\hat \\beta = (X^TX)^{-1}X^Ty)? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!","headline":"Derivation: Ordinary Least Squares Solution and the Normal Equations","mainEntityOfPage":{"@type":"WebPage","@id":"https://dustinstansbury.github.io/theclevermachine/derivation-normal-equations"},"url":"https://dustinstansbury.github.io/theclevermachine/derivation-normal-equations"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://dustinstansbury.github.io/theclevermachine/feed.xml" title="The Clever Machine" /><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171913050-1');
</script>

  

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/theclevermachine/">The Clever Machine</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/theclevermachine/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Derivation: Ordinary Least Squares Solution and the Normal Equations</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-07-23T00:00:00-07:00" itemprop="datePublished">Jul 23, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">By Dustin Stansbury </span></span><br><span itemprop="tags">
        
        
          
              <a href="/theclevermachine/tags/ordinary-least-squares.html">ordinary-least-squares</a>
              , 
          
              <a href="/theclevermachine/tags/derivation.html">derivation</a>
              , 
          
              <a href="/theclevermachine/tags/normal-equations.html">normal-equations</a>
              
          
        

      </span></p>

  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Have you ever performed linear regression involving multiple predictor variables and run into this expression \(\hat \beta = (X^TX)^{-1}X^Ty\)? It’s called the OLS solution via Normal Equations. To find out where it comes from, read on!</p>

<p>In the linear regression framework, we model an output variable \(y\) (in this case a scalar) as a linear combination of some independent input variables \(X\) plus some independent noise \(\epsilon\). The linear combination of the independent variables is defined by a parameter vector \(\beta\):</p>

\[y = X \beta + \epsilon\]

<p>We also assume that the noise term \(\epsilon\) is drawn from a Normal distribution with zero mean and a noise variance \(\sigma_{\epsilon}^2\) (generally assumed to be equal to one):</p>

\[\epsilon \sim N(0,\sigma_{\epsilon}^2)\]

<p>For some estimate of the model parameters \(\hat \beta\), the model’s prediction errors (a.k.a. <em>residuals</em>) \(e\) are the difference between the model prediction and the observed ouput values:</p>

\[e = y - X\hat \beta\]

<p>The <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares (OLS) solution</a> to the problem–i.e. determining an optimal solution for \(\hat \beta\)–requires minimizing the sum of the squared errors with respect to the model parameters \(\hat \beta\). It turns out, the sum of squared errors <a href="https://en.wikipedia.org/wiki/Dot_product">is equal to the inner product of the residuals vector with itself</a> \(\sum_i e_i^2 = e^Te\) :</p>

\[\begin{align}
 e^T e &amp;= (y - X \hat \beta)^T (y - X \hat \beta) \\  
 &amp;= y^Ty - y^T (X \hat \beta) - (X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\
 &amp;= y^Ty - (X \hat \beta)^T y - (X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\
 &amp;= y^Ty - 2(X \hat \beta)^T y + (X \hat \beta)^T (X \hat \beta) \\
 &amp;= y^Ty - 2\hat \beta^T X^T y + \hat \beta^T X^T X \hat \beta \text{,} \tag{1}
\end{align}\]

<p>where we take advantage of the matrix identity \((AB)^T = B^TA^T\) in steps 2-3 above.</p>

<p>To determine the parameters \(\hat \beta\) we minimize the sum of squared errors with respect to the parameters:</p>

\[\begin{align}
\frac{\partial}{\partial \beta} \left[ e^T e \right] &amp;= 0 \\
\frac{\partial}{\partial \beta}  \left[ y^Ty - 2\hat \beta^T X^T y + \hat \beta^T X^T X \hat \beta \right ] &amp;= 0 \;\; \text{, via Eq. (1)}\\
-2X^Ty + 2X^TX \hat \beta &amp;= 0 \\ 
-X^Ty + X^TX \hat \beta &amp;= 0 \\ 
X^TX \hat \beta&amp;= X^Ty  \text{,} \tag{2}

\end{align}\]

<p>where we note to the matrix derivative identity \(\frac{\partial \mathbf{a}^T \mathbf{b}}{\partial \mathbf{a}} = \mathbf{b}\), for vectors \(\mathbf{a}\) and \(\mathbf{b}\) in step 2-3 above.</p>

<p>The relationship in <strong><em>Equation 2</em></strong> is the matrix form of what are known as the <a href="https://mathworld.wolfram.com/NormalEquation.html">Normal Equations</a>. Solving for \(\hat \beta\) gives the analytical solution to the Ordinary Least Squares problem.</p>

\[\hat \beta = (X^TX)^{-1}X^Ty\]

<p>…and voila!</p>

<hr />
<hr />
<h1 id="notes">Notes</h1>
<p>This post is a refactor of content with the same title originally posted on <a href="https://theclevermachine.wordpress.com/2012/09/01/derivation-of-ols-normal-equations/">The Clever Machine Wordpress blog</a>.</p>

  </div><div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'the-clever-machine'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/theclevermachine/derivation-normal-equations" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/theclevermachine/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The Clever Machine</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Dustin Stansbury, PhD</li><li><a class="u-email" href="mailto:[first_name][dot][last_name][at][google email][dotcom]">[first_name][dot][last_name][at][google email][dotcom]</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/dustinstansbury"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#github"></use></svg> <span class="username">dustinstansbury</span></a></li><li><a href="https://www.twitter.com/corrcoef"><svg class="svg-icon"><use xlink:href="/theclevermachine/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">corrcoef</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Musings on data and science</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
